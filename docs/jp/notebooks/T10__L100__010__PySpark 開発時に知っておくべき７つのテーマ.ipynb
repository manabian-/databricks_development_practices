{"cells":[{"cell_type":"markdown","source":["# PySpark 開発時に知っておくべき７つのテーマ"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"466b5e24-0989-423b-a5d1-98357c5cfd2d"}}},{"cell_type":"markdown","source":["PySpark 開発時に知っておくべき７つのテーマである次の項目を説明する。\n\n1. SparkSession 、 Spark Dataframe 、および、 Spark table\n2. `Dataframe in, Dataframe out`\n3. 変数とクラスの利用\n4. 変数からデータフレームを作成\n5. データフレームにおけるメタデータとデータを取得\n6. Spark SQL を PySpark で利用\n7. 制御フローによる PySpark処理の実行"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0c4100b-1a36-400f-bd5d-d54989c1e5de"}}},{"cell_type":"markdown","source":["## 前提知識"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11f96b7e-94b0-4c10-bb43-4b2a4151222e"}}},{"cell_type":"markdown","source":["- Spark （PySpark、Spark SQL）の基本的な知識\n- データエンジニアリング の基本的な知識\n- Python の基本的な知識"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52579376-f10f-44bb-b598-b8efc2e936fc"}}},{"cell_type":"markdown","source":["## 1. SparkSession 、 Spark Dataframe 、および、 Spark table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"adf64148-8689-4cc8-8501-4cefe07b2c68"}}},{"cell_type":"markdown","source":["### 1-1. SparkSession概要"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"35005a02-f5e4-45a3-ad26-6d99e1edd500"}}},{"cell_type":"markdown","source":["Sparkの処理は、`SparkSesson`にデータ処理を定義していき、アクションによりデータ処理が行われること（遅延評価）が特徴。Databricks 利用時にはノートブックをクラスターにアタッチする際に自動で定義されるため意識することはあまりないが、ノートブック外からテスト実行を行う際には手動の定義が必要となる。遅延評価により、処理内容の分割・共通化を分割することで単体テスト実施に合わせた構成とすることができ、システムにおける品質や拡張性が高くなる。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69067ec8-1a15-4553-8126-bb7171fd5082"}}},{"cell_type":"code","source":["# Databricks では、ノートブックのアタッチ時に自動で`Spark`という変数に`SparkSession`が設定される\nspark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8246790e-8811-4810-bd35-733476ac50d1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[1]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[1]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=6672612882213712#setting/sparkui/0401-001605-ka2e9r5g/driver-1564885301607180269\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://10.139.64.4:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=6672612882213712#setting/sparkui/0401-001605-ka2e9r5g/driver-1564885301607180269\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://10.139.64.4:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":0},{"cell_type":"code","source":["# 外部から実行する場合には、`SparkSession.builder.getOrCreate`にて定義することが前提\n# 定義済みの`SparkSession`がある場合には、それを取得\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\nspark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e082c443-a36f-4b78-9cd4-fb7e0b4d2d82"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[2]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=6672612882213712#setting/sparkui/0401-001605-ka2e9r5g/driver-1564885301607180269\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://10.139.64.4:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=6672612882213712#setting/sparkui/0401-001605-ka2e9r5g/driver-1564885301607180269\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.2.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://10.139.64.4:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":0},{"cell_type":"code","source":["data = [\n    {'int_col': '1'},\n]\n\n# `SparkSession`に処理（データの読み込み）を追加\nspark.createDataFrame(data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"04801bf8-489f-413c-8dfb-b9c7603a3194"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[3]: DataFrame[int_col: string]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: DataFrame[int_col: string]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# `show`というアクションを含めることでデータ処理(`Spark jobs`)が実行される\nspark.createDataFrame(data).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e212520-9135-4aba-aee7-69aca7db666c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+\n|int_col|\n+-------+\n|      1|\n+-------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+\nint_col|\n+-------+\n      1|\n+-------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 1-2. `SparkSession` を再利用した関数（メソッド）の定義"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"86392c12-54e1-4cfc-9d3b-6cc6c202d04e"}}},{"cell_type":"markdown","source":["Spark処理開始時に`SparkSession`を定義して、処理を共通化した関数内で`getActiveSession`により定義済みの`SparkSession`を取得。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fce9e458-e921-4f71-9306-40fbc4c6d12c"}}},{"cell_type":"code","source":["# データフレームを作成するメソッドを定義\nfrom pyspark.sql import SparkSession\ndef create_test_DataFrame():\n    # `getActiveSession`にて定義済みの`SparkSession`が取得のみも可能\n    spark = SparkSession.getActiveSession()\n    data = [\n        {'int_col': '1'},\n    ]\n    return spark.createDataFrame(data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4686dc11-2353-406a-b08a-327262ecfb8a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# `SparkSession`を定義\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n# メソッドを利用してデータを表示\ncreate_test_DataFrame().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ea77d37e-3edf-4d68-a630-630542ed8c49"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+\n|int_col|\n+-------+\n|      1|\n+-------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+\nint_col|\n+-------+\n      1|\n+-------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 1-3. Spark.conf の設定"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa2a89d2-c668-4ad7-bf2a-161b55a5a08a"}}},{"cell_type":"markdown","source":["Spark の処理実行時のパラメータとして`spark.conf`があり、`SparkSession`にて設定を変更可能。\n\n- [Configuration - Spark 3.3.0 Documentation (apache.org)](https://spark.apache.org/docs/latest/configuration.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4df9f5a-7ccb-41ca-bebb-f849b922ec20"}}},{"cell_type":"code","source":["try:\n    spark.conf.get('spark.databricks.delta.schema.autoMerge')\nexcept:\n    print('not set')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17c8a01b-369a-4cc5-8984-62b90a6fa387"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">not set\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">not set\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# `spark.conf.set`にて値を設定\nspark.conf.set('spark.databricks.delta.schema.autoMerge', 'true')\nprint(spark.conf.get('spark.databricks.delta.schema.autoMerge'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ecc7d8b-8f2c-4f49-9aba-21b6e63b2dd1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">true\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">true\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 設定をリセット\nspark.conf.unset('spark.databricks.delta.schema.autoMerge')\ntry:\n    spark.conf.get('spark.databricks.delta.schema.autoMerge')\nexcept:\n    print('not set')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"093942c3-efe0-4e67-a443-7d15add4c16a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">not set\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">not set\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Spark SQL でも設定可能\nspark.sql('set spark.databricks.delta.schema.autoMerge = true')\nprint(spark.conf.get('spark.databricks.delta.schema.autoMerge'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf4fec9b-8996-4ae1-8bea-26235ab2f692"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">true\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">true\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 1-4. SparkSession の並列化処理時の注意事項"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4642a23a-172f-449b-9645-3d79aee003b6"}}},{"cell_type":"markdown","source":["定義した`SparkSession`を multiprocessing などの並列化ライブラリーで共有すると正常に動作しないことがあるため、処理ごとに`SparkSession`の定義が必要となる。<br>\nDatabricks では`dbutils.notebook.run()`メソッドの利用が推奨されているなど、Spark プロバイダーごとに対応方法が異なる。\n\n- [ノートブックでコードをモジュール化またはリンクする - Azure Databricks | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/databricks/notebooks/notebook-workflows)\n- [Microsoft Spark Utilities の概要 - Azure Synapse Analytics | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-python#notebook-utilities)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c6593bed-2ca7-4dfd-8494-360e67089881"}}},{"cell_type":"markdown","source":["### 1-5. Spark table の永続化"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1645b185-32c0-4dbb-a065-d4d87a99d02c"}}},{"cell_type":"markdown","source":["Spark tableは、メタストアに格納されることで、セッション終了後にも利用すること（テーブルの永続化）ができる。\n\nメタストアとして、 Hive メタストアが利用されることが多いが、Spark プロバイダー固有のメタストアが提供されていることもある。\n\n- [Unity Catalog (プレビュー) - Azure Databricks | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/databricks/data-governance/unity-catalog/)\n- [共有メタデータ モデル - Azure Synapse Analytics | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/synapse-analytics/metadata/overview)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfe67fe3-f57f-4eb8-96c4-77771bf26e31"}}},{"cell_type":"markdown","source":["### 1-6. テーブルとデータの分離"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5cf7d202-e366-47bd-af3c-dee5f7768b04"}}},{"cell_type":"markdown","source":["データレイクを理解するために、テーブルとデータが分離されているという概念を理解することが重要である。リレーショナルデータベース（RDB）ではテーブル = データとなっていることが多く、RDBに慣れている方はテーブルを管理すればデータも管理できると考える傾向にある。データレイクでは、１つのデータ（ディレクトリ）に複数のテーブルが紐づくことから、テーブルで管理する方法では重複操作が実施される可能性がある。管理単位を、テーブルとして捉えるのではなく、ディレクトリとして捉える必要がある。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97e019ae-7eb4-4d42-ba4a-5519785ab8ef"}}},{"cell_type":"code","source":["# データベースとテーブルを作成\ndb_name = 'T10__L100__010'\nspark.sql(f'''\nCREATE DATABASE IF NOT EXISTS {db_name}\n''')\n\ntbl_name = 'nation'\ntbl_full_name = f'{db_name}.{tbl_name}'\n\nspark.sql(f'''\nCREATE OR REPLACE TABLE {tbl_full_name}\n(\n  N_NATIONKEY  integer\n  ,N_NAME       string\n  ,N_REGIONKEY  integer \n  ,N_COMMENT    string\n)\nUSING delta\n''')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"092291e7-cb9e-49df-8d38-632b66324bbd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[11]: DataFrame[]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[11]: DataFrame[]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["tbl_desc = spark.sql(f\"DESC TABLE EXTENDED {tbl_full_name}\")\n\ntbl_desc.display()\ntbl_location = (\n    tbl_desc\n    .filter('col_name = \"Location\"')\n    .select(\"data_type\")\n    .collect()[0][0]\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ff98800a-f9b7-450e-a937-f8b44a1d1d10"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["N_NATIONKEY","int",""],["N_NAME","string",""],["N_REGIONKEY","int",""],["N_COMMENT","string",""],["","",""],["# Partitioning","",""],["Not partitioned","",""],["","",""],["# Detailed Table Information","",""],["Catalog","spark_catalog",""],["Database","T10__L100__010",""],["Table","nation",""],["Location","dbfs:/user/hive/warehouse/t10__l100__010.db/nation",""],["Provider","delta",""],["Owner","root",""],["Is_managed_location","true",""],["Type","MANAGED",""],["Table Properties","[delta.minReaderVersion=1,delta.minWriterVersion=2]",""]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"col_name","type":"\"string\"","metadata":"{\"comment\":\"name of the column\"}"},{"name":"data_type","type":"\"string\"","metadata":"{\"comment\":\"data type of the column\"}"},{"name":"comment","type":"\"string\"","metadata":"{\"comment\":\"comment of the column\"}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>N_NATIONKEY</td><td>int</td><td></td></tr><tr><td>N_NAME</td><td>string</td><td></td></tr><tr><td>N_REGIONKEY</td><td>int</td><td></td></tr><tr><td>N_COMMENT</td><td>string</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td># Partitioning</td><td></td><td></td></tr><tr><td>Not partitioned</td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td># Detailed Table Information</td><td></td><td></td></tr><tr><td>Catalog</td><td>spark_catalog</td><td></td></tr><tr><td>Database</td><td>T10__L100__010</td><td></td></tr><tr><td>Table</td><td>nation</td><td></td></tr><tr><td>Location</td><td>dbfs:/user/hive/warehouse/t10__l100__010.db/nation</td><td></td></tr><tr><td>Provider</td><td>delta</td><td></td></tr><tr><td>Owner</td><td>root</td><td></td></tr><tr><td>Is_managed_location</td><td>true</td><td></td></tr><tr><td>Type</td><td>MANAGED</td><td></td></tr><tr><td>Table Properties</td><td>[delta.minReaderVersion=1,delta.minWriterVersion=2]</td><td></td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# テーブルへのデータ書き込み\nfilepath = \"dbfs:/databricks-datasets/tpch/data-001/nation/\"\n\nschema = \"\"\"\n  N_NATIONKEY  integer\n  ,N_NAME       string\n  ,N_REGIONKEY  integer \n  ,N_COMMENT    string\n\"\"\"\n   \ndf = (spark\n       .read\n       .format(\"csv\")\n       .schema(schema)\n       .option(\"inferSchema\", \"True\")\n       .option(\"sep\", \"|\")\n       .load(filepath)\n    )\ndf.write.format('delta').mode('overwrite').saveAsTable(tbl_full_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89425b7a-9aba-48ae-a3ff-09a8f83dc797"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# データとディレクトリの確認\nspark.table(tbl_full_name).display()\ndisplay(dbutils.fs.ls(tbl_location))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0568f88a-60df-4d02-a9c7-8bf47b3792a2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[0,"ALGERIA",0," haggle. carefully final deposits detect slyly agai"],[1,"ARGENTINA",1,"al foxes promise slyly according to the regular accounts. bold requests alon"],[2,"BRAZIL",1,"y alongside of the pending deposits. carefully special packages are about the ironic forges. slyly special "],[3,"CANADA",1,"eas hang ironic, silent packages. slyly regular packages are furiously over the tithes. fluffily bold"],[4,"EGYPT",4,"y above the carefully unusual theodolites. final dugouts are quickly across the furiously regular d"],[5,"ETHIOPIA",0,"ven packages wake quickly. regu"],[6,"FRANCE",3,"refully final requests. regular, ironi"],[7,"GERMANY",3,"l platelets. regular accounts x-ray: unusual, regular acco"],[8,"INDIA",2,"ss excuses cajole slyly across the packages. deposits print aroun"],[9,"INDONESIA",2," slyly express asymptotes. regular deposits haggle slyly. carefully ironic hockey players sleep blithely. carefull"],[10,"IRAN",4,"efully alongside of the slyly final dependencies. "],[11,"IRAQ",4,"nic deposits boost atop the quickly final requests? quickly regula"],[12,"JAPAN",2,"ously. final, express gifts cajole a"],[13,"JORDAN",4,"ic deposits are blithely about the carefully regular pa"],[14,"KENYA",0," pending excuses haggle furiously deposits. pending, express pinto beans wake fluffily past t"],[15,"MOROCCO",0,"rns. blithely bold courts among the closely regular packages use furiously bold platelets?"],[16,"MOZAMBIQUE",0,"s. ironic, unusual asymptotes wake blithely r"],[17,"PERU",1,"platelets. blithely pending dependencies use fluffily across the even pinto beans. carefully silent accoun"],[18,"CHINA",2,"c dependencies. furiously express notornis sleep slyly regular accounts. ideas sleep. depos"],[19,"ROMANIA",3,"ular asymptotes are about the furious multipliers. express dependencies nag above the ironically ironic account"],[20,"SAUDI ARABIA",4,"ts. silent requests haggle. closely express packages sleep across the blithely"],[21,"VIETNAM",2,"hely enticingly express accounts. even, final "],[22,"RUSSIA",3," requests against the platelets use never according to the quickly regular pint"],[23,"UNITED KINGDOM",3,"eans boost carefully special requests. accounts are. carefull"],[24,"UNITED STATES",1,"y final packages. slow foxes cajole quickly. quickly silent platelets breach ironic accounts. unusual pinto be"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"N_NATIONKEY","type":"\"integer\"","metadata":"{}"},{"name":"N_NAME","type":"\"string\"","metadata":"{}"},{"name":"N_REGIONKEY","type":"\"integer\"","metadata":"{}"},{"name":"N_COMMENT","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>N_NATIONKEY</th><th>N_NAME</th><th>N_REGIONKEY</th><th>N_COMMENT</th></tr></thead><tbody><tr><td>0</td><td>ALGERIA</td><td>0</td><td> haggle. carefully final deposits detect slyly agai</td></tr><tr><td>1</td><td>ARGENTINA</td><td>1</td><td>al foxes promise slyly according to the regular accounts. bold requests alon</td></tr><tr><td>2</td><td>BRAZIL</td><td>1</td><td>y alongside of the pending deposits. carefully special packages are about the ironic forges. slyly special </td></tr><tr><td>3</td><td>CANADA</td><td>1</td><td>eas hang ironic, silent packages. slyly regular packages are furiously over the tithes. fluffily bold</td></tr><tr><td>4</td><td>EGYPT</td><td>4</td><td>y above the carefully unusual theodolites. final dugouts are quickly across the furiously regular d</td></tr><tr><td>5</td><td>ETHIOPIA</td><td>0</td><td>ven packages wake quickly. regu</td></tr><tr><td>6</td><td>FRANCE</td><td>3</td><td>refully final requests. regular, ironi</td></tr><tr><td>7</td><td>GERMANY</td><td>3</td><td>l platelets. regular accounts x-ray: unusual, regular acco</td></tr><tr><td>8</td><td>INDIA</td><td>2</td><td>ss excuses cajole slyly across the packages. deposits print aroun</td></tr><tr><td>9</td><td>INDONESIA</td><td>2</td><td> slyly express asymptotes. regular deposits haggle slyly. carefully ironic hockey players sleep blithely. carefull</td></tr><tr><td>10</td><td>IRAN</td><td>4</td><td>efully alongside of the slyly final dependencies. </td></tr><tr><td>11</td><td>IRAQ</td><td>4</td><td>nic deposits boost atop the quickly final requests? quickly regula</td></tr><tr><td>12</td><td>JAPAN</td><td>2</td><td>ously. final, express gifts cajole a</td></tr><tr><td>13</td><td>JORDAN</td><td>4</td><td>ic deposits are blithely about the carefully regular pa</td></tr><tr><td>14</td><td>KENYA</td><td>0</td><td> pending excuses haggle furiously deposits. pending, express pinto beans wake fluffily past t</td></tr><tr><td>15</td><td>MOROCCO</td><td>0</td><td>rns. blithely bold courts among the closely regular packages use furiously bold platelets?</td></tr><tr><td>16</td><td>MOZAMBIQUE</td><td>0</td><td>s. ironic, unusual asymptotes wake blithely r</td></tr><tr><td>17</td><td>PERU</td><td>1</td><td>platelets. blithely pending dependencies use fluffily across the even pinto beans. carefully silent accoun</td></tr><tr><td>18</td><td>CHINA</td><td>2</td><td>c dependencies. furiously express notornis sleep slyly regular accounts. ideas sleep. depos</td></tr><tr><td>19</td><td>ROMANIA</td><td>3</td><td>ular asymptotes are about the furious multipliers. express dependencies nag above the ironically ironic account</td></tr><tr><td>20</td><td>SAUDI ARABIA</td><td>4</td><td>ts. silent requests haggle. closely express packages sleep across the blithely</td></tr><tr><td>21</td><td>VIETNAM</td><td>2</td><td>hely enticingly express accounts. even, final </td></tr><tr><td>22</td><td>RUSSIA</td><td>3</td><td> requests against the platelets use never according to the quickly regular pint</td></tr><tr><td>23</td><td>UNITED KINGDOM</td><td>3</td><td>eans boost carefully special requests. accounts are. carefull</td></tr><tr><td>24</td><td>UNITED STATES</td><td>1</td><td>y final packages. slow foxes cajole quickly. quickly silent platelets breach ironic accounts. unusual pinto be</td></tr></tbody></table></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["dbfs:/user/hive/warehouse/t10__l100__010.db/nation/_delta_log/","_delta_log/",0,1664892005000],["dbfs:/user/hive/warehouse/t10__l100__010.db/nation/part-00000-58af55d9-f569-4b4b-a54b-36007e334ca7-c000.snappy.parquet","part-00000-58af55d9-f569-4b4b-a54b-36007e334ca7-c000.snappy.parquet",3103,1664892002000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"},{"name":"modificationTime","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/user/hive/warehouse/t10__l100__010.db/nation/_delta_log/</td><td>_delta_log/</td><td>0</td><td>1664892005000</td></tr><tr><td>dbfs:/user/hive/warehouse/t10__l100__010.db/nation/part-00000-58af55d9-f569-4b4b-a54b-36007e334ca7-c000.snappy.parquet</td><td>part-00000-58af55d9-f569-4b4b-a54b-36007e334ca7-c000.snappy.parquet</td><td>3103</td><td>1664892002000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 既存のデータからテーブルを作成\ntbl_name_2 = 'nation_2'\ntbl_full_name_2 = f'{db_name}.{tbl_name_2}'\n\nspark.sql(f'''\nDROP TABLE IF EXISTS {tbl_full_name_2}\n''')\nspark.sql(f'''\nCREATE TABLE {tbl_full_name_2}\nUSING delta\nLOCATION '{tbl_location}'\n''')\n\nprint(spark.sql(f'SHOW CREATE TABLE {tbl_full_name_2}').collect()[0][0])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92d842fa-066d-49ec-8dfe-3866c80c9749"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">CREATE TABLE spark_catalog.t10__l100__010.nation_2 (\n  N_NATIONKEY INT,\n  N_NAME STRING,\n  N_REGIONKEY INT,\n  N_COMMENT STRING)\nUSING delta\nLOCATION &#39;dbfs:/user/hive/warehouse/t10__l100__010.db/nation&#39;\nTBLPROPERTIES (\n  &#39;Type&#39; = &#39;EXTERNAL&#39;,\n  &#39;delta.minReaderVersion&#39; = &#39;1&#39;,\n  &#39;delta.minWriterVersion&#39; = &#39;2&#39;)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">CREATE TABLE spark_catalog.t10__l100__010.nation_2 (\n  N_NATIONKEY INT,\n  N_NAME STRING,\n  N_REGIONKEY INT,\n  N_COMMENT STRING)\nUSING delta\nLOCATION &#39;dbfs:/user/hive/warehouse/t10__l100__010.db/nation&#39;\nTBLPROPERTIES (\n  &#39;Type&#39; = &#39;EXTERNAL&#39;,\n  &#39;delta.minReaderVersion&#39; = &#39;1&#39;,\n  &#39;delta.minWriterVersion&#39; = &#39;2&#39;)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# テーブルにデータを書き込んでしまうと。。。\ndf.write.format('delta').mode('append').saveAsTable(tbl_full_name_2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4efe5ab6-2d52-43e0-8114-63ea7ae606a0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 同じデータが書き込まれてることを確認\nspark.table(f'delta.`{tbl_location}`').display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19ee152a-d063-4999-87ec-ed1a011df5c5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[0,"ALGERIA",0," haggle. carefully final deposits detect slyly agai"],[1,"ARGENTINA",1,"al foxes promise slyly according to the regular accounts. bold requests alon"],[2,"BRAZIL",1,"y alongside of the pending deposits. carefully special packages are about the ironic forges. slyly special "],[3,"CANADA",1,"eas hang ironic, silent packages. slyly regular packages are furiously over the tithes. fluffily bold"],[4,"EGYPT",4,"y above the carefully unusual theodolites. final dugouts are quickly across the furiously regular d"],[5,"ETHIOPIA",0,"ven packages wake quickly. regu"],[6,"FRANCE",3,"refully final requests. regular, ironi"],[7,"GERMANY",3,"l platelets. regular accounts x-ray: unusual, regular acco"],[8,"INDIA",2,"ss excuses cajole slyly across the packages. deposits print aroun"],[9,"INDONESIA",2," slyly express asymptotes. regular deposits haggle slyly. carefully ironic hockey players sleep blithely. carefull"],[10,"IRAN",4,"efully alongside of the slyly final dependencies. "],[11,"IRAQ",4,"nic deposits boost atop the quickly final requests? quickly regula"],[12,"JAPAN",2,"ously. final, express gifts cajole a"],[13,"JORDAN",4,"ic deposits are blithely about the carefully regular pa"],[14,"KENYA",0," pending excuses haggle furiously deposits. pending, express pinto beans wake fluffily past t"],[15,"MOROCCO",0,"rns. blithely bold courts among the closely regular packages use furiously bold platelets?"],[16,"MOZAMBIQUE",0,"s. ironic, unusual asymptotes wake blithely r"],[17,"PERU",1,"platelets. blithely pending dependencies use fluffily across the even pinto beans. carefully silent accoun"],[18,"CHINA",2,"c dependencies. furiously express notornis sleep slyly regular accounts. ideas sleep. depos"],[19,"ROMANIA",3,"ular asymptotes are about the furious multipliers. express dependencies nag above the ironically ironic account"],[20,"SAUDI ARABIA",4,"ts. silent requests haggle. closely express packages sleep across the blithely"],[21,"VIETNAM",2,"hely enticingly express accounts. even, final "],[22,"RUSSIA",3," requests against the platelets use never according to the quickly regular pint"],[23,"UNITED KINGDOM",3,"eans boost carefully special requests. accounts are. carefull"],[24,"UNITED STATES",1,"y final packages. slow foxes cajole quickly. quickly silent platelets breach ironic accounts. unusual pinto be"],[0,"ALGERIA",0," haggle. carefully final deposits detect slyly agai"],[1,"ARGENTINA",1,"al foxes promise slyly according to the regular accounts. bold requests alon"],[2,"BRAZIL",1,"y alongside of the pending deposits. carefully special packages are about the ironic forges. slyly special "],[3,"CANADA",1,"eas hang ironic, silent packages. slyly regular packages are furiously over the tithes. fluffily bold"],[4,"EGYPT",4,"y above the carefully unusual theodolites. final dugouts are quickly across the furiously regular d"],[5,"ETHIOPIA",0,"ven packages wake quickly. regu"],[6,"FRANCE",3,"refully final requests. regular, ironi"],[7,"GERMANY",3,"l platelets. regular accounts x-ray: unusual, regular acco"],[8,"INDIA",2,"ss excuses cajole slyly across the packages. deposits print aroun"],[9,"INDONESIA",2," slyly express asymptotes. regular deposits haggle slyly. carefully ironic hockey players sleep blithely. carefull"],[10,"IRAN",4,"efully alongside of the slyly final dependencies. "],[11,"IRAQ",4,"nic deposits boost atop the quickly final requests? quickly regula"],[12,"JAPAN",2,"ously. final, express gifts cajole a"],[13,"JORDAN",4,"ic deposits are blithely about the carefully regular pa"],[14,"KENYA",0," pending excuses haggle furiously deposits. pending, express pinto beans wake fluffily past t"],[15,"MOROCCO",0,"rns. blithely bold courts among the closely regular packages use furiously bold platelets?"],[16,"MOZAMBIQUE",0,"s. ironic, unusual asymptotes wake blithely r"],[17,"PERU",1,"platelets. blithely pending dependencies use fluffily across the even pinto beans. carefully silent accoun"],[18,"CHINA",2,"c dependencies. furiously express notornis sleep slyly regular accounts. ideas sleep. depos"],[19,"ROMANIA",3,"ular asymptotes are about the furious multipliers. express dependencies nag above the ironically ironic account"],[20,"SAUDI ARABIA",4,"ts. silent requests haggle. closely express packages sleep across the blithely"],[21,"VIETNAM",2,"hely enticingly express accounts. even, final "],[22,"RUSSIA",3," requests against the platelets use never according to the quickly regular pint"],[23,"UNITED KINGDOM",3,"eans boost carefully special requests. accounts are. carefull"],[24,"UNITED STATES",1,"y final packages. slow foxes cajole quickly. quickly silent platelets breach ironic accounts. unusual pinto be"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"N_NATIONKEY","type":"\"integer\"","metadata":"{}"},{"name":"N_NAME","type":"\"string\"","metadata":"{}"},{"name":"N_REGIONKEY","type":"\"integer\"","metadata":"{}"},{"name":"N_COMMENT","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>N_NATIONKEY</th><th>N_NAME</th><th>N_REGIONKEY</th><th>N_COMMENT</th></tr></thead><tbody><tr><td>0</td><td>ALGERIA</td><td>0</td><td> haggle. carefully final deposits detect slyly agai</td></tr><tr><td>1</td><td>ARGENTINA</td><td>1</td><td>al foxes promise slyly according to the regular accounts. bold requests alon</td></tr><tr><td>2</td><td>BRAZIL</td><td>1</td><td>y alongside of the pending deposits. carefully special packages are about the ironic forges. slyly special </td></tr><tr><td>3</td><td>CANADA</td><td>1</td><td>eas hang ironic, silent packages. slyly regular packages are furiously over the tithes. fluffily bold</td></tr><tr><td>4</td><td>EGYPT</td><td>4</td><td>y above the carefully unusual theodolites. final dugouts are quickly across the furiously regular d</td></tr><tr><td>5</td><td>ETHIOPIA</td><td>0</td><td>ven packages wake quickly. regu</td></tr><tr><td>6</td><td>FRANCE</td><td>3</td><td>refully final requests. regular, ironi</td></tr><tr><td>7</td><td>GERMANY</td><td>3</td><td>l platelets. regular accounts x-ray: unusual, regular acco</td></tr><tr><td>8</td><td>INDIA</td><td>2</td><td>ss excuses cajole slyly across the packages. deposits print aroun</td></tr><tr><td>9</td><td>INDONESIA</td><td>2</td><td> slyly express asymptotes. regular deposits haggle slyly. carefully ironic hockey players sleep blithely. carefull</td></tr><tr><td>10</td><td>IRAN</td><td>4</td><td>efully alongside of the slyly final dependencies. </td></tr><tr><td>11</td><td>IRAQ</td><td>4</td><td>nic deposits boost atop the quickly final requests? quickly regula</td></tr><tr><td>12</td><td>JAPAN</td><td>2</td><td>ously. final, express gifts cajole a</td></tr><tr><td>13</td><td>JORDAN</td><td>4</td><td>ic deposits are blithely about the carefully regular pa</td></tr><tr><td>14</td><td>KENYA</td><td>0</td><td> pending excuses haggle furiously deposits. pending, express pinto beans wake fluffily past t</td></tr><tr><td>15</td><td>MOROCCO</td><td>0</td><td>rns. blithely bold courts among the closely regular packages use furiously bold platelets?</td></tr><tr><td>16</td><td>MOZAMBIQUE</td><td>0</td><td>s. ironic, unusual asymptotes wake blithely r</td></tr><tr><td>17</td><td>PERU</td><td>1</td><td>platelets. blithely pending dependencies use fluffily across the even pinto beans. carefully silent accoun</td></tr><tr><td>18</td><td>CHINA</td><td>2</td><td>c dependencies. furiously express notornis sleep slyly regular accounts. ideas sleep. depos</td></tr><tr><td>19</td><td>ROMANIA</td><td>3</td><td>ular asymptotes are about the furious multipliers. express dependencies nag above the ironically ironic account</td></tr><tr><td>20</td><td>SAUDI ARABIA</td><td>4</td><td>ts. silent requests haggle. closely express packages sleep across the blithely</td></tr><tr><td>21</td><td>VIETNAM</td><td>2</td><td>hely enticingly express accounts. even, final </td></tr><tr><td>22</td><td>RUSSIA</td><td>3</td><td> requests against the platelets use never according to the quickly regular pint</td></tr><tr><td>23</td><td>UNITED KINGDOM</td><td>3</td><td>eans boost carefully special requests. accounts are. carefull</td></tr><tr><td>24</td><td>UNITED STATES</td><td>1</td><td>y final packages. slow foxes cajole quickly. quickly silent platelets breach ironic accounts. unusual pinto be</td></tr><tr><td>0</td><td>ALGERIA</td><td>0</td><td> haggle. carefully final deposits detect slyly agai</td></tr><tr><td>1</td><td>ARGENTINA</td><td>1</td><td>al foxes promise slyly according to the regular accounts. bold requests alon</td></tr><tr><td>2</td><td>BRAZIL</td><td>1</td><td>y alongside of the pending deposits. carefully special packages are about the ironic forges. slyly special </td></tr><tr><td>3</td><td>CANADA</td><td>1</td><td>eas hang ironic, silent packages. slyly regular packages are furiously over the tithes. fluffily bold</td></tr><tr><td>4</td><td>EGYPT</td><td>4</td><td>y above the carefully unusual theodolites. final dugouts are quickly across the furiously regular d</td></tr><tr><td>5</td><td>ETHIOPIA</td><td>0</td><td>ven packages wake quickly. regu</td></tr><tr><td>6</td><td>FRANCE</td><td>3</td><td>refully final requests. regular, ironi</td></tr><tr><td>7</td><td>GERMANY</td><td>3</td><td>l platelets. regular accounts x-ray: unusual, regular acco</td></tr><tr><td>8</td><td>INDIA</td><td>2</td><td>ss excuses cajole slyly across the packages. deposits print aroun</td></tr><tr><td>9</td><td>INDONESIA</td><td>2</td><td> slyly express asymptotes. regular deposits haggle slyly. carefully ironic hockey players sleep blithely. carefull</td></tr><tr><td>10</td><td>IRAN</td><td>4</td><td>efully alongside of the slyly final dependencies. </td></tr><tr><td>11</td><td>IRAQ</td><td>4</td><td>nic deposits boost atop the quickly final requests? quickly regula</td></tr><tr><td>12</td><td>JAPAN</td><td>2</td><td>ously. final, express gifts cajole a</td></tr><tr><td>13</td><td>JORDAN</td><td>4</td><td>ic deposits are blithely about the carefully regular pa</td></tr><tr><td>14</td><td>KENYA</td><td>0</td><td> pending excuses haggle furiously deposits. pending, express pinto beans wake fluffily past t</td></tr><tr><td>15</td><td>MOROCCO</td><td>0</td><td>rns. blithely bold courts among the closely regular packages use furiously bold platelets?</td></tr><tr><td>16</td><td>MOZAMBIQUE</td><td>0</td><td>s. ironic, unusual asymptotes wake blithely r</td></tr><tr><td>17</td><td>PERU</td><td>1</td><td>platelets. blithely pending dependencies use fluffily across the even pinto beans. carefully silent accoun</td></tr><tr><td>18</td><td>CHINA</td><td>2</td><td>c dependencies. furiously express notornis sleep slyly regular accounts. ideas sleep. depos</td></tr><tr><td>19</td><td>ROMANIA</td><td>3</td><td>ular asymptotes are about the furious multipliers. express dependencies nag above the ironically ironic account</td></tr><tr><td>20</td><td>SAUDI ARABIA</td><td>4</td><td>ts. silent requests haggle. closely express packages sleep across the blithely</td></tr><tr><td>21</td><td>VIETNAM</td><td>2</td><td>hely enticingly express accounts. even, final </td></tr><tr><td>22</td><td>RUSSIA</td><td>3</td><td> requests against the platelets use never according to the quickly regular pint</td></tr><tr><td>23</td><td>UNITED KINGDOM</td><td>3</td><td>eans boost carefully special requests. accounts are. carefull</td></tr><tr><td>24</td><td>UNITED STATES</td><td>1</td><td>y final packages. slow foxes cajole quickly. quickly silent platelets breach ironic accounts. unusual pinto be</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.sql(f'''\nDROP DATABASE {db_name} CASCADE\n''')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f14f5bd-e23b-416f-a578-a7a2e2682385"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[18]: DataFrame[]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[18]: DataFrame[]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 2. `Dataframe in, Dataframe out`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2262d7f-eee7-4114-b468-07f6048a5a3d"}}},{"cell_type":"markdown","source":["Spark のデータエンジニアリングを実施する際には、`Dataframe in, Dataframe out`、つまり、データフレームを引数としてデータフレームをリターンとするメソッドを組み合わせて開発を行うことが推奨されている。その理由の１つは、テストのやりやすさがある。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8550c0dc-013d-4a89-9d6a-428da5339c08"}}},{"cell_type":"markdown","source":["## 2-1. `Dataframe in, Dataframe out`の実施例"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fc0b4ccd-0d2b-4739-b76d-2b599b39ea5d"}}},{"cell_type":"code","source":["# 1で作成したデータフレームの`int_col`が`string`型であるため、`int` 型にする必要がある\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf = create_test_DataFrame()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1867a28f-39e5-4155-86c9-473583ee2371"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# データフレームの`int_col`カラムを`integer`型に変更する関数を定義\ndef cast_int_col_as_int(\n    df,\n):\n    return df.withColumn('int_col', df.int_col.cast('int'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"284aace9-7edb-42ef-8c2d-b66be6adf73c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 関数により`int_col`カラムが`integer`型となることを確認\ncast_int_col_as_int(df).printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"092a5c3d-433c-4eb2-aef4-214c9b8602e5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- int_col: integer (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- int_col: integer (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 2-2. 単体テストの実施例"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5fbcc278-8ebf-4684-b25d-643ef35b0d71"}}},{"cell_type":"code","source":["# 関数に対する単体テストを実行\n\nimport unittest\nfrom pyspark.sql import SparkSession\n\nclass Test__cast_int_col_as_int(unittest.TestCase):\n    \"\"\"`cast_int_col_as_int`関数に対する単体テスト\"\"\"\n    def test__cast_int_col_as_int(self):\n        \"\"\"\n        正常系検証\n        \"\"\"\n        # テストデータを準備\n        spark = SparkSession.builder.getOrCreate()\n        data = [\n            {'int_col': '1'},\n            {'int_col': '12.3'},\n            {'int_col': 'string'},\n        ]\n        test_df = spark.createDataFrame(data)\n        \n        # 想定結果を準備\n        expect_date =[\n            {'int_col': 1},\n            {'int_col': 12},\n            {'int_col': None},\n        ]\n        expected_schema = '''\n        int_col integer\n        '''\n        expected_result = spark.createDataFrame(expect_date,expected_schema)\n        \n        # テスト対象の関数を実行\n        result_df = cast_int_col_as_int(test_df)\n\n        # テストの実行結果を確認\n        self.assertEqual(\n            result_df.collect(),\n            expected_result.collect(),\n        )\n\n#\nsuite = unittest.TestLoader().loadTestsFromTestCase(Test__cast_int_col_as_int)\nrunner = unittest.TextTestRunner(\n    verbosity=2,\n)\nrunner.run(suite)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12acea7d-b0e9-44ab-b3a8-17441e2a667e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">test__cast_int_col_as_int (__main__.Test__cast_int_col_as_int)\n正常系検証 ... ok\n\n----------------------------------------------------------------------\nRan 1 test in 0.560s\n\nOK\nOut[22]: &lt;unittest.runner.TextTestResult run=1 errors=0 failures=0&gt;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">test__cast_int_col_as_int (__main__.Test__cast_int_col_as_int)\n正常系検証 ... ok\n\n----------------------------------------------------------------------\nRan 1 test in 0.560s\n\nOK\nOut[22]: &lt;unittest.runner.TextTestResult run=1 errors=0 failures=0&gt;</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 3. 変数とクラス"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f07ed49c-c060-445e-ab02-0fe4541c17c1"}}},{"cell_type":"markdown","source":["### 3-1. Spark Dataframeの不変性"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03229cce-aeb0-475f-ba02-b926ff2f3b91"}}},{"cell_type":"markdown","source":["PySpark における Spark データフレームは不変であるという特徴があり、処理の定義をリスト型変数や辞書型変数に格納できる。１つの変数に格納することで、開発時のデバックの実施が容易となる。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca658271-5284-4d0f-9903-2e47461fef52"}}},{"cell_type":"code","source":["# `spark_dataframes` 変数に定義した Spark データフレームを格納\n\nspark_dataframes = {}\ndf = create_test_DataFrame()\nspark_dataframes['first_dataframe'] = df\ndf_2 = cast_int_col_as_int(df)\nspark_dataframes['second_dataframe'] = df_2"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a31b3c95-5341-4bae-aac3-80ffdab1ccd6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# `spark_dataframes`にて保持しているデータフレームのスキーマをループ処理で表示\nprint(spark_dataframes)\nfor spark_dataframe in spark_dataframes.values():\n    spark_dataframe.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01d6034c-c3f4-4c53-9ea6-2e0373bea6ea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">{&#39;first_dataframe&#39;: DataFrame[int_col: string], &#39;second_dataframe&#39;: DataFrame[int_col: int]}\nroot\n |-- int_col: string (nullable = true)\n\nroot\n |-- int_col: integer (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{&#39;first_dataframe&#39;: DataFrame[int_col: string], &#39;second_dataframe&#39;: DataFrame[int_col: int]}\nroot\n-- int_col: string (nullable = true)\n\nroot\n-- int_col: integer (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 3-2. f-stringによりリテラルの置換"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"694eb4f5-eb28-4462-b7f8-69b7b1bb2e52"}}},{"cell_type":"code","source":["view_name = 'test'\n\ncreate_test_DataFrame().createOrReplaceTempView(view_name)\n\nspark.sql(f'''\nSELECT\n  *\n  FROM\n    {view_name}\n''').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5d8c52b-35ac-4801-b0f1-eac4bc0e3011"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+\n|int_col|\n+-------+\n|      1|\n+-------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+\nint_col|\n+-------+\n      1|\n+-------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 3-3. Spark SQLにて変数を利用"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3816b25-1f6c-433e-a8b5-90d0cdfe0f34"}}},{"cell_type":"markdown","source":["spark.conf 経由で変数を渡すことが可能。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3f9811b3-3de0-49e6-9bd5-00582e233a4f"}}},{"cell_type":"code","source":["spark.conf.set('dq.val.view_name', view_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce9942f7-2fbc-4cb0-afd8-24ac2d943018"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\n\nSELECT\n  *\n  FROM \n    ${dq.val.view_name}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a937c673-1bf5-4cb9-8655-357beb5f7b30"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["1"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"int_col","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>int_col</th></tr></thead><tbody><tr><td>1</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 3-4. staticmethod によるメソッドの整理"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69028b4b-71b1-46b2-9f6b-338f898ce8da"}}},{"cell_type":"markdown","source":["ノートブック型の開発環境では`%run`コマンドにより他ファイルで定義したクラスや関数を読み込むことができるが、関数をクラスの`staticmethod` （暗黙の第一引数を受け取らないメソッド）として定義することで管理が容易となる。\n\n参考リンク\n\n- [@staticmethod](https://docs.python.org/ja/3.8/library/functions.html#staticmethod)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9912a59-351e-449e-a4ee-aa398ba63f89"}}},{"cell_type":"code","source":["class DataEngineeringSample:\n    @staticmethod\n    def create_test_DataFrame():\n        # `getActiveSession`にて定義済みの`SparkSession`が取得のみも可能\n        spark = SparkSession.getActiveSession()\n        data = [\n            {'int_col': '1'},\n        ]\n\n        # `SparkSession`に処理（データの読み込み）を追加\n        return spark.createDataFrame(data)    \n    \n    @staticmethod\n    def cast_int_col_as_int(\n        df,\n    ):\n        return df.withColumn('int_col', df.int_col.cast('int'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a072d84e-9535-4e6b-bcb8-3f03e740dbb3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["DataEngineeringSample.create_test_DataFrame().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"00119731-fcfa-4480-a86c-04671eec60bc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+\n|int_col|\n+-------+\n|      1|\n+-------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+\nint_col|\n+-------+\n      1|\n+-------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 3-5. クラス変数を config として利用"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a93f7030-cbdb-4ee6-919c-bba3e96d02cb"}}},{"cell_type":"code","source":["# クラス変数を保持したクラスを定義\nclass SparkUtilities:\n    spark_provider = 'databricks'\n    @staticmethod\n    def display(tgt_df):\n        if SparkUtilities.spark_provider == 'databricks':\n            tgt_df.display()\n        if SparkUtilities.spark_provider == 'apache_spark':\n            tgt_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2bde40a4-1df9-45c7-8e62-232903505af7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# クラス変数を変更して処理内容を変更\nSparkUtilities.spark_provider = 'apache_spark'\nSparkUtilities.display(\n    spark.createDataFrame([{'int_col': '1'},]),\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97741998-839b-4197-a7eb-d40ecf77890c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+\n|int_col|\n+-------+\n|      1|\n+-------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+\nint_col|\n+-------+\n      1|\n+-------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 4. 変数からデータフレームを作成"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6da7a29a-1aa1-4d2b-b29c-7598bd63d620"}}},{"cell_type":"markdown","source":["変数からデータフレームを作成する主な方法としては、次の３種類がある。単体テストのデータを用意する際などに利用する。\n\n1. 辞書型リストを用いる方法\n2. 多次元リストを用いる方法\n3. `pyspark.sql.Row`メソッドを用いる方法"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4ccbce02-d04d-47af-b72d-4d478cb94d48"}}},{"cell_type":"markdown","source":["###　4-1. 辞書型リストを用いる方法"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dbf7caaa-4c1d-4c9d-bee7-52506ade95c3"}}},{"cell_type":"code","source":["import datetime\n\nschema = '''\nstr_col string,\nint_col integer,\ndate_col date\n'''\n\ndata_001 = [\n    {\n        'str_col': 'abc',\n        'int_col': 123,\n        'date_col': datetime.date(2020, 1, 1),\n    },\n    {\n        'str_col': 'def',\n        'int_col': 456,\n        'date_col': datetime.date(2020, 1, 2),\n    },\n]\n\ndf = spark.createDataFrame(\n    data_001,\n    schema,\n)\n\n\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f564d0e6-f273-4bd6-9e0c-68924743c154"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+-------+----------+\n|str_col|int_col|  date_col|\n+-------+-------+----------+\n|    abc|    123|2020-01-01|\n|    def|    456|2020-01-02|\n+-------+-------+----------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-------+----------+\nstr_col|int_col|  date_col|\n+-------+-------+----------+\n    abc|    123|2020-01-01|\n    def|    456|2020-01-02|\n+-------+-------+----------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["辞書型リストを用いる方法を、`collect`メソッドにより簡単に確認できる。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5ecc5eb1-beab-402b-9cc5-128458379bf5"}}},{"cell_type":"code","source":["df_2 = spark.createDataFrame(\n    df.collect(),\n    schema,\n)\n\ndf_dict = []\nfor row_data in df.collect():\n    df_dict.append(row_data.asDict())\n\ndf_2.show()\nprint(df_dict)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c2e7eac-8b26-4ade-9712-9a4c9e9b4ed4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+-------+----------+\n|str_col|int_col|  date_col|\n+-------+-------+----------+\n|    abc|    123|2020-01-01|\n|    def|    456|2020-01-02|\n+-------+-------+----------+\n\n[{&#39;str_col&#39;: &#39;abc&#39;, &#39;int_col&#39;: 123, &#39;date_col&#39;: datetime.date(2020, 1, 1)}, {&#39;str_col&#39;: &#39;def&#39;, &#39;int_col&#39;: 456, &#39;date_col&#39;: datetime.date(2020, 1, 2)}]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-------+----------+\nstr_col|int_col|  date_col|\n+-------+-------+----------+\n    abc|    123|2020-01-01|\n    def|    456|2020-01-02|\n+-------+-------+----------+\n\n[{&#39;str_col&#39;: &#39;abc&#39;, &#39;int_col&#39;: 123, &#39;date_col&#39;: datetime.date(2020, 1, 1)}, {&#39;str_col&#39;: &#39;def&#39;, &#39;int_col&#39;: 456, &#39;date_col&#39;: datetime.date(2020, 1, 2)}]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["###　4-2. 多次元リストを用いる方法"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d46d6b5e-ab6a-4e69-875b-fbaeeaafe890"}}},{"cell_type":"code","source":["import datetime\n\nschema = '''\nstr_col string,\nint_col integer,\ndate_col date\n'''\n\ndata_002 = [\n    [\n        'abc',\n        123,\n        datetime.date(2020, 1, 1)\n    ],\n    [\n        'def',\n        456,\n        datetime.date(2020, 1, 2)\n    ],\n]\n\ndf = spark.createDataFrame(\n    data_002,\n    schema,\n)\n\n\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81d48f05-7d29-4675-8386-5ce28e81c255"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+-------+----------+\n|str_col|int_col|  date_col|\n+-------+-------+----------+\n|    abc|    123|2020-01-01|\n|    def|    456|2020-01-02|\n+-------+-------+----------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-------+----------+\nstr_col|int_col|  date_col|\n+-------+-------+----------+\n    abc|    123|2020-01-01|\n    def|    456|2020-01-02|\n+-------+-------+----------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# tuple in list でも同様のことが可能\nimport datetime\n\nschema = '''\nstr_col string,\nint_col integer,\ndate_col date\n'''\n\ndata_002 = [\n    (\n        'abc',\n        123,\n        datetime.date(2020, 1, 1)\n    ),\n    (\n        'def',\n        456,\n        datetime.date(2020, 1, 2)\n    ),\n]\n\ndf = spark.createDataFrame(\n    data_002,\n    schema,\n)\n\n\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5503086f-7e6e-4f0a-8272-ba3ee366850a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+-------+----------+\n|str_col|int_col|  date_col|\n+-------+-------+----------+\n|    abc|    123|2020-01-01|\n|    def|    456|2020-01-02|\n+-------+-------+----------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-------+----------+\nstr_col|int_col|  date_col|\n+-------+-------+----------+\n    abc|    123|2020-01-01|\n    def|    456|2020-01-02|\n+-------+-------+----------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["###　4-3. `pyspark.sql.Row`メソッドを用いる方法"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f5bf4ff-d32d-45af-bed1-6271f9daa1cc"}}},{"cell_type":"code","source":["import datetime\nfrom pyspark.sql import Row\n\nschema = '''\nstr_col string,\nint_col integer,\ndate_col date\n'''\n\ndata_003 = [\n    Row(\n        str_col='abc',\n        int_col=123,\n        date_col=datetime.date(2020, 1, 1),\n    ),\n    Row(\n        str_col='def',\n        int_col=456,\n        date_col=datetime.date(2020, 1, 2),\n    ),\n]\n\ndf = spark.createDataFrame(\n    data_003,\n    schema,\n)\n\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"683f1783-feee-4b37-a89b-2534cf85444f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+-------+----------+\n|str_col|int_col|  date_col|\n+-------+-------+----------+\n|    abc|    123|2020-01-01|\n|    def|    456|2020-01-02|\n+-------+-------+----------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-------+----------+\nstr_col|int_col|  date_col|\n+-------+-------+----------+\n    abc|    123|2020-01-01|\n    def|    456|2020-01-02|\n+-------+-------+----------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["`pyspark.sql.Row`の値の設定方法を、`collect`メソッドにより簡単に確認できる。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97402a91-8f9f-4981-8807-4300c513fc94"}}},{"cell_type":"code","source":["df_2 = spark.createDataFrame(\n    df.collect(),\n    schema,\n)\n\ndf_2.show()\nprint(df.collect())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7551203e-71b5-40c2-8ac1-97b5ec01d9a4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+-------+----------+\n|str_col|int_col|  date_col|\n+-------+-------+----------+\n|    abc|    123|2020-01-01|\n|    def|    456|2020-01-02|\n+-------+-------+----------+\n\n[Row(str_col=&#39;abc&#39;, int_col=123, date_col=datetime.date(2020, 1, 1)), Row(str_col=&#39;def&#39;, int_col=456, date_col=datetime.date(2020, 1, 2))]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-------+----------+\nstr_col|int_col|  date_col|\n+-------+-------+----------+\n    abc|    123|2020-01-01|\n    def|    456|2020-01-02|\n+-------+-------+----------+\n\n[Row(str_col=&#39;abc&#39;, int_col=123, date_col=datetime.date(2020, 1, 1)), Row(str_col=&#39;def&#39;, int_col=456, date_col=datetime.date(2020, 1, 2))]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 4-4. Spark のデータ型と Python 型の対応"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f64592e8-eb44-4e86-ae50-2f251a8a9290"}}},{"cell_type":"markdown","source":["Spark データフレームのデータ型に対応する Python のデータ型は次のドキュメントに記載。\n- [Data Types - Spark 3.3.0 Documentation (apache.org)](https://spark.apache.org/docs/latest/sql-ref-datatypes.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5aca9a1b-24a3-4097-b77f-3af695d9f6a6"}}},{"cell_type":"code","source":["# サンプルデータフレームを作成\nfrom pyspark.sql import Row\nfrom decimal import *\nimport datetime\n\nsample_Data = [\n    Row(\n        string_column='AAA',\n        byte_column=1,\n        integer_column=1,\n        bigint_column=1,\n        float_column=12.300000190734863,\n        double_column=12.3,\n        numeric_column=Decimal('12'),\n        boolean_column=True,\n        date_column=datetime.date(2020, 1, 1),\n        timestamp_column=datetime.datetime(2021, 1, 1, 0, 0),\n        binary_column=bytearray(b'A'),\n        struct_column=Row(struct_string_column='AAA', struct_int_column=1),\n        array_column=['AAA', 'BBB', 'CCC'],\n        map_column={'AAA': 1},\n    )\n]\nschema = '''\n--文字型\nstring_column string,\n\n--整数型\nbyte_column byte,\ninteger_column integer,\nbigint_column bigint,\n\n--浮動小数点型\nfloat_column float,\ndouble_column double,\nnumeric_column numeric,\n\n--真偽型\nboolean_column boolean,\n\n--日付時刻\ndate_column date, \ntimestamp_column timestamp,\n\n--バイナリー型\nbinary_column binary,\n\n--複合型\nstruct_column struct<\n    struct_string_column :string,\n    struct_int_column    :int\n>,\narray_column array<string>, \nmap_column map<string, int>\n'''\n\nspark.createDataFrame(sample_Data, schema).show()\nspark.createDataFrame(sample_Data, schema).printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f996704-8680-46b0-adb1-1535cb0abdc1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------------+-----------+--------------+-------------+------------+-------------+--------------+--------------+-----------+-------------------+-------------+-------------+---------------+----------+\n|string_column|byte_column|integer_column|bigint_column|float_column|double_column|numeric_column|boolean_column|date_column|   timestamp_column|binary_column|struct_column|   array_column|map_column|\n+-------------+-----------+--------------+-------------+------------+-------------+--------------+--------------+-----------+-------------------+-------------+-------------+---------------+----------+\n|          AAA|          1|             1|            1|        12.3|         12.3|            12|          true| 2020-01-01|2021-01-01 00:00:00|         [41]|     {AAA, 1}|[AAA, BBB, CCC]|{AAA -&gt; 1}|\n+-------------+-----------+--------------+-------------+------------+-------------+--------------+--------------+-----------+-------------------+-------------+-------------+---------------+----------+\n\nroot\n |-- string_column: string (nullable = true)\n |-- byte_column: byte (nullable = true)\n |-- integer_column: integer (nullable = true)\n |-- bigint_column: long (nullable = true)\n |-- float_column: float (nullable = true)\n |-- double_column: double (nullable = true)\n |-- numeric_column: decimal(10,0) (nullable = true)\n |-- boolean_column: boolean (nullable = true)\n |-- date_column: date (nullable = true)\n |-- timestamp_column: timestamp (nullable = true)\n |-- binary_column: binary (nullable = true)\n |-- struct_column: struct (nullable = true)\n |    |-- struct_string_column: string (nullable = true)\n |    |-- struct_int_column: integer (nullable = true)\n |-- array_column: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- map_column: map (nullable = true)\n |    |-- key: string\n |    |-- value: integer (valueContainsNull = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+-----------+--------------+-------------+------------+-------------+--------------+--------------+-----------+-------------------+-------------+-------------+---------------+----------+\nstring_column|byte_column|integer_column|bigint_column|float_column|double_column|numeric_column|boolean_column|date_column|   timestamp_column|binary_column|struct_column|   array_column|map_column|\n+-------------+-----------+--------------+-------------+------------+-------------+--------------+--------------+-----------+-------------------+-------------+-------------+---------------+----------+\n          AAA|          1|             1|            1|        12.3|         12.3|            12|          true| 2020-01-01|2021-01-01 00:00:00|         [41]|     {AAA, 1}|[AAA, BBB, CCC]|{AAA -&gt; 1}|\n+-------------+-----------+--------------+-------------+------------+-------------+--------------+--------------+-----------+-------------------+-------------+-------------+---------------+----------+\n\nroot\n-- string_column: string (nullable = true)\n-- byte_column: byte (nullable = true)\n-- integer_column: integer (nullable = true)\n-- bigint_column: long (nullable = true)\n-- float_column: float (nullable = true)\n-- double_column: double (nullable = true)\n-- numeric_column: decimal(10,0) (nullable = true)\n-- boolean_column: boolean (nullable = true)\n-- date_column: date (nullable = true)\n-- timestamp_column: timestamp (nullable = true)\n-- binary_column: binary (nullable = true)\n-- struct_column: struct (nullable = true)\n    |-- struct_string_column: string (nullable = true)\n    |-- struct_int_column: integer (nullable = true)\n-- array_column: array (nullable = true)\n    |-- element: string (containsNull = true)\n-- map_column: map (nullable = true)\n    |-- key: string\n    |-- value: integer (valueContainsNull = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 5. データフレームにおけるメタデータとデータを取得"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9d826647-cf90-4f18-8e2f-afef4c0834a0"}}},{"cell_type":"markdown","source":["Spark データフレームからカラム名等のメタデータを、次のメソッドを用いることで取得できる。２つのデータフレームを比較してその差分を変更するメソッドを開発する際などに利用。\n\n- `DataFrame.schema.jsonValue()['fields']`\n- `DataFrame.columns`\n- `DataFrame.dtypes`\n\n\nScala のみで利用できる DDL 文字列を取得する場合には、次のように記載する。\n\n```\njson_data = df.schema.json()\nddl_string = spark.sparkContext._jvm.org.apache.spark.sql.types.DataType.fromJson(json_data).toDDL()\n`````"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a47baba-b16f-4380-b15b-7288de5da1a3"}}},{"cell_type":"code","source":["# サンプルデータフレームを作成\nfrom pyspark.sql import Row\nfrom decimal import *\nimport datetime\n\nsample_Data = [\n    Row(\n        string_column='AAA',\n        byte_column=1,\n        integer_column=1,\n        bigint_column=1,\n        float_column=12.300000190734863,\n        double_column=12.3,\n        numeric_column=Decimal('12'),\n        boolean_column=True,\n        date_column=datetime.date(2020, 1, 1),\n        timestamp_column=datetime.datetime(2021, 1, 1, 0, 0),\n        binary_column=bytearray(b'A'),\n        struct_column=Row(struct_string_column='AAA', struct_int_column=1),\n        array_column=['AAA', 'BBB', 'CCC'],\n        map_column={'AAA': 1},\n    )\n]\nschema = '''\n--文字型\nstring_column string,\n\n--整数型\nbyte_column byte,\ninteger_column integer,\nbigint_column bigint,\n\n--浮動小数点型\nfloat_column float,\ndouble_column double,\nnumeric_column numeric,\n\n--真偽型\nboolean_column boolean,\n\n--日付時刻\ndate_column date, \ntimestamp_column timestamp,\n\n--バイナリー型\nbinary_column binary,\n\n--複合型\nstruct_column struct<\n    struct_string_column :string,\n    struct_int_column    :int\n>,\narray_column array<string>, \nmap_column map<string, int>\n'''\n\ndf = spark.createDataFrame(sample_Data, schema)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a1a8eb1-c0c3-4b2c-8e85-5a04146f9ef4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 5-1. カラムの詳細情報を取得"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d83633ff-0db0-4fc7-b1f3-dfc34bb5155e"}}},{"cell_type":"code","source":["\ndf.schema.jsonValue()['fields']"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f55bfb3-9e9c-47a3-a69c-90ed9cfbacf9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[40]: [{&#39;name&#39;: &#39;string_column&#39;, &#39;type&#39;: &#39;string&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;byte_column&#39;, &#39;type&#39;: &#39;byte&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;integer_column&#39;,\n  &#39;type&#39;: &#39;integer&#39;,\n  &#39;nullable&#39;: True,\n  &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;bigint_column&#39;, &#39;type&#39;: &#39;long&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;float_column&#39;, &#39;type&#39;: &#39;float&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;double_column&#39;, &#39;type&#39;: &#39;double&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;numeric_column&#39;,\n  &#39;type&#39;: &#39;decimal(10,0)&#39;,\n  &#39;nullable&#39;: True,\n  &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;boolean_column&#39;,\n  &#39;type&#39;: &#39;boolean&#39;,\n  &#39;nullable&#39;: True,\n  &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;date_column&#39;, &#39;type&#39;: &#39;date&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;timestamp_column&#39;,\n  &#39;type&#39;: &#39;timestamp&#39;,\n  &#39;nullable&#39;: True,\n  &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;binary_column&#39;, &#39;type&#39;: &#39;binary&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;struct_column&#39;,\n  &#39;type&#39;: {&#39;type&#39;: &#39;struct&#39;,\n   &#39;fields&#39;: [{&#39;name&#39;: &#39;struct_string_column&#39;,\n     &#39;type&#39;: &#39;string&#39;,\n     &#39;nullable&#39;: True,\n     &#39;metadata&#39;: {}},\n    {&#39;name&#39;: &#39;struct_int_column&#39;,\n     &#39;type&#39;: &#39;integer&#39;,\n     &#39;nullable&#39;: True,\n     &#39;metadata&#39;: {}}]},\n  &#39;nullable&#39;: True,\n  &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;array_column&#39;,\n  &#39;type&#39;: {&#39;type&#39;: &#39;array&#39;, &#39;elementType&#39;: &#39;string&#39;, &#39;containsNull&#39;: True},\n  &#39;nullable&#39;: True,\n  &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;map_column&#39;,\n  &#39;type&#39;: {&#39;type&#39;: &#39;map&#39;,\n   &#39;keyType&#39;: &#39;string&#39;,\n   &#39;valueType&#39;: &#39;integer&#39;,\n   &#39;valueContainsNull&#39;: True},\n  &#39;nullable&#39;: True,\n  &#39;metadata&#39;: {}}]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[40]: [{&#39;name&#39;: &#39;string_column&#39;, &#39;type&#39;: &#39;string&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;byte_column&#39;, &#39;type&#39;: &#39;byte&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;integer_column&#39;,\n  &#39;type&#39;: &#39;integer&#39;,\n  &#39;nullable&#39;: True,\n  &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;bigint_column&#39;, &#39;type&#39;: &#39;long&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;float_column&#39;, &#39;type&#39;: &#39;float&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;double_column&#39;, &#39;type&#39;: &#39;double&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;numeric_column&#39;,\n  &#39;type&#39;: &#39;decimal(10,0)&#39;,\n  &#39;nullable&#39;: True,\n  &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;boolean_column&#39;,\n  &#39;type&#39;: &#39;boolean&#39;,\n  &#39;nullable&#39;: True,\n  &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;date_column&#39;, &#39;type&#39;: &#39;date&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;timestamp_column&#39;,\n  &#39;type&#39;: &#39;timestamp&#39;,\n  &#39;nullable&#39;: True,\n  &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;binary_column&#39;, &#39;type&#39;: &#39;binary&#39;, &#39;nullable&#39;: True, &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;struct_column&#39;,\n  &#39;type&#39;: {&#39;type&#39;: &#39;struct&#39;,\n   &#39;fields&#39;: [{&#39;name&#39;: &#39;struct_string_column&#39;,\n     &#39;type&#39;: &#39;string&#39;,\n     &#39;nullable&#39;: True,\n     &#39;metadata&#39;: {}},\n    {&#39;name&#39;: &#39;struct_int_column&#39;,\n     &#39;type&#39;: &#39;integer&#39;,\n     &#39;nullable&#39;: True,\n     &#39;metadata&#39;: {}}]},\n  &#39;nullable&#39;: True,\n  &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;array_column&#39;,\n  &#39;type&#39;: {&#39;type&#39;: &#39;array&#39;, &#39;elementType&#39;: &#39;string&#39;, &#39;containsNull&#39;: True},\n  &#39;nullable&#39;: True,\n  &#39;metadata&#39;: {}},\n {&#39;name&#39;: &#39;map_column&#39;,\n  &#39;type&#39;: {&#39;type&#39;: &#39;map&#39;,\n   &#39;keyType&#39;: &#39;string&#39;,\n   &#39;valueType&#39;: &#39;integer&#39;,\n   &#39;valueContainsNull&#39;: True},\n  &#39;nullable&#39;: True,\n  &#39;metadata&#39;: {}}]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 5-2. カラム名のリストを取得"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"95ed3660-f14f-4e83-a304-ab99a76c8460"}}},{"cell_type":"code","source":["df.columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"71a26534-6ba0-4c24-aac6-e73ac2504d23"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[41]: [&#39;string_column&#39;,\n &#39;byte_column&#39;,\n &#39;integer_column&#39;,\n &#39;bigint_column&#39;,\n &#39;float_column&#39;,\n &#39;double_column&#39;,\n &#39;numeric_column&#39;,\n &#39;boolean_column&#39;,\n &#39;date_column&#39;,\n &#39;timestamp_column&#39;,\n &#39;binary_column&#39;,\n &#39;struct_column&#39;,\n &#39;array_column&#39;,\n &#39;map_column&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[41]: [&#39;string_column&#39;,\n &#39;byte_column&#39;,\n &#39;integer_column&#39;,\n &#39;bigint_column&#39;,\n &#39;float_column&#39;,\n &#39;double_column&#39;,\n &#39;numeric_column&#39;,\n &#39;boolean_column&#39;,\n &#39;date_column&#39;,\n &#39;timestamp_column&#39;,\n &#39;binary_column&#39;,\n &#39;struct_column&#39;,\n &#39;array_column&#39;,\n &#39;map_column&#39;]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 5-3. カラム名とデータ型のタプルのリストを取得"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6409d4d9-7a41-4e3b-bf42-8d998b42091a"}}},{"cell_type":"code","source":["df.dtypes"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eba2d497-e9ba-4461-a532-5daa7bde09a5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[42]: [(&#39;string_column&#39;, &#39;string&#39;),\n (&#39;byte_column&#39;, &#39;tinyint&#39;),\n (&#39;integer_column&#39;, &#39;int&#39;),\n (&#39;bigint_column&#39;, &#39;bigint&#39;),\n (&#39;float_column&#39;, &#39;float&#39;),\n (&#39;double_column&#39;, &#39;double&#39;),\n (&#39;numeric_column&#39;, &#39;decimal(10,0)&#39;),\n (&#39;boolean_column&#39;, &#39;boolean&#39;),\n (&#39;date_column&#39;, &#39;date&#39;),\n (&#39;timestamp_column&#39;, &#39;timestamp&#39;),\n (&#39;binary_column&#39;, &#39;binary&#39;),\n (&#39;struct_column&#39;,\n  &#39;struct&lt;struct_string_column:string,struct_int_column:int&gt;&#39;),\n (&#39;array_column&#39;, &#39;array&lt;string&gt;&#39;),\n (&#39;map_column&#39;, &#39;map&lt;string,int&gt;&#39;)]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[42]: [(&#39;string_column&#39;, &#39;string&#39;),\n (&#39;byte_column&#39;, &#39;tinyint&#39;),\n (&#39;integer_column&#39;, &#39;int&#39;),\n (&#39;bigint_column&#39;, &#39;bigint&#39;),\n (&#39;float_column&#39;, &#39;float&#39;),\n (&#39;double_column&#39;, &#39;double&#39;),\n (&#39;numeric_column&#39;, &#39;decimal(10,0)&#39;),\n (&#39;boolean_column&#39;, &#39;boolean&#39;),\n (&#39;date_column&#39;, &#39;date&#39;),\n (&#39;timestamp_column&#39;, &#39;timestamp&#39;),\n (&#39;binary_column&#39;, &#39;binary&#39;),\n (&#39;struct_column&#39;,\n  &#39;struct&lt;struct_string_column:string,struct_int_column:int&gt;&#39;),\n (&#39;array_column&#39;, &#39;array&lt;string&gt;&#39;),\n (&#39;map_column&#39;, &#39;map&lt;string,int&gt;&#39;)]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 5-4. DDL 文字列の取得"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08a9b1fa-1d1e-4f53-b215-e860ef6ef8b3"}}},{"cell_type":"code","source":["json_data = df.schema.json()\nddl_string = spark.sparkContext._jvm.org.apache.spark.sql.types.DataType.fromJson(json_data).toDDL()\nprint(ddl_string)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"836ba049-18f8-4375-8eb1-c2db978396e3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">string_column STRING,byte_column TINYINT,integer_column INT,bigint_column BIGINT,float_column FLOAT,double_column DOUBLE,numeric_column DECIMAL(10,0),boolean_column BOOLEAN,date_column DATE,timestamp_column TIMESTAMP,binary_column BINARY,struct_column STRUCT&lt;struct_string_column: STRING, struct_int_column: INT&gt;,array_column ARRAY&lt;STRING&gt;,map_column MAP&lt;STRING, INT&gt;\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">string_column STRING,byte_column TINYINT,integer_column INT,bigint_column BIGINT,float_column FLOAT,double_column DOUBLE,numeric_column DECIMAL(10,0),boolean_column BOOLEAN,date_column DATE,timestamp_column TIMESTAMP,binary_column BINARY,struct_column STRUCT&lt;struct_string_column: STRING, struct_int_column: INT&gt;,array_column ARRAY&lt;STRING&gt;,map_column MAP&lt;STRING, INT&gt;\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 5-5. データフレームの単一のデータを取得"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec2f2000-4c40-4093-9e1f-5ced002e5cf8"}}},{"cell_type":"markdown","source":["`collect`メソッドを利用するため、事前にデータフレーム操作によりデータを絞ることが重要"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"616b7dee-38af-45ee-a42a-fb3bc3f9629b"}}},{"cell_type":"code","source":["# データを準備\nimport datetime\nfrom pyspark.sql import Row\n\nsample_Data = [\n    Row(\n        string_column='AAA',\n        bigint_column=1,\n        date_column=datetime.date(2020, 1, 1),\n    ),\n    Row(\n        string_column='bbb',\n        bigint_column=2,\n        date_column=datetime.date(2020, 2, 2),\n    ),\n    Row(\n        string_column='CCC',\n        bigint_column=3,\n        date_column=datetime.date(2020, 3, 3),\n    ),\n]\n\ndf = spark.createDataFrame(sample_Data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2fbec120-c6ac-4c02-a52f-17bc6d373277"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# データを絞る\nsingle_value_df = (\n    df\n    .filter('string_column = \"AAA\"')\n    .select('string_column')\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52357bb7-e76c-4c65-85fa-f588e68a1c7c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# データフレームの単一値を取得\n# 位置で指定する方法\nprint(single_value_df.collect()[0][0])\n\n# カラム名をキーとして指定\nprint(single_value_df.collect()[0]['string_column'])\n\n# カラム名を指定\nprint(single_value_df.collect()[0].string_column)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74c9f145-8af3-4c6d-bf4e-00dadd5b70e2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">AAA\nAAA\nAAA\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">AAA\nAAA\nAAA\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 5-6. データフレームの複数のデータを取得"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50af4214-308b-4f44-8146-3368eb07dfa6"}}},{"cell_type":"code","source":["# データを準備\nimport datetime\nfrom pyspark.sql import Row\n\nsample_Data = [\n    Row(\n        string_column='AAA',\n        bigint_column=1,\n        date_column=datetime.date(2020, 1, 1),\n    ),\n    Row(\n        string_column='bbb',\n        bigint_column=2,\n        date_column=datetime.date(2020, 2, 2),\n    ),\n    Row(\n        string_column='CCC',\n        bigint_column=3,\n        date_column=datetime.date(2020, 3, 3),\n    ),\n]\n\ndf = spark.createDataFrame(sample_Data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"09a40ab8-b048-4b78-bdd3-014897ffda91"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# データフレームの複数値を取得\n\n# 位置で指定する方法\nmulti_values_df = (\n    df\n    .select('string_column')\n)\n\ndf_values = []\nfor row_data in multi_values_df.collect():\n    df_values.append(row_data[0])\nprint(df_values)\n\n\n# カラム名をキーとして指定\ntgt_col = 'string_column'\ndf_values = []\nfor row_data in df.collect():\n    df_values.append(row_data[tgt_col])\nprint(df_values)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aefc2c70-e795-4aac-b841-ae39c89f75df"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[&#39;AAA&#39;, &#39;bbb&#39;, &#39;CCC&#39;]\n[&#39;AAA&#39;, &#39;bbb&#39;, &#39;CCC&#39;]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;AAA&#39;, &#39;bbb&#39;, &#39;CCC&#39;]\n[&#39;AAA&#39;, &#39;bbb&#39;, &#39;CCC&#39;]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 6. Spark SQL を PySpark で利用"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d47b9d27-529e-49c2-9ed2-d20b02fcd07c"}}},{"cell_type":"markdown","source":["PySparkでは主に`pyspark.sql.functions`を利用するのですが、次のメソッドでは Spark SQL(Spark SQL, Built-in Functions)が利用可能。\n\n- spark.sql\n- selectExpr\n- expr\n\n次のメソッドでは、SQL とほぼ同じ記法が利用可能。\n\n- filter\n\nデータフレームに別名（alias）を設定する際には次のメソッドを利用\n\n- alias"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55059a6d-6212-4f14-989b-e8aa2c9aef62"}}},{"cell_type":"markdown","source":["### 6-1. `spark.sql`の利用例"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"44c448dd-ea44-4a9d-bc1b-8fbdef5c1985"}}},{"cell_type":"code","source":["df = spark.sql('''\nSELECT 1 as int_col\nUNION ALL\nSELECT 12.3 as int_col\nUNION ALL\nSELECT \"string\" as int_col\n''')\n\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0f081153-327f-41da-b991-f75922838661"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+\n|int_col|\n+-------+\n|    1.0|\n|   12.3|\n| string|\n+-------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+\nint_col|\n+-------+\n    1.0|\n   12.3|\n string|\n+-------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 6-2. `selectExpr`の利用例"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32b9411d-48c3-43bd-839b-0994e16ed1cc"}}},{"cell_type":"code","source":["df.selectExpr('CAST(int_col as int) as int_col').show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f790ef0c-1af1-4f8e-b029-281cedfa14aa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+\n|int_col|\n+-------+\n|      1|\n|     12|\n|   null|\n+-------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+\nint_col|\n+-------+\n      1|\n     12|\n   null|\n+-------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 6-3. `expr`の利用例"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d067e6a-6d7f-4f05-b14c-a033621328f8"}}},{"cell_type":"code","source":["from pyspark.sql.functions import expr\ndf.withColumn('int_col', expr('CAST(int_col as int)')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8b93dbb-2133-4cf8-a4bb-dd390f3abbf8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+\n|int_col|\n+-------+\n|      1|\n|     12|\n|   null|\n+-------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+\nint_col|\n+-------+\n      1|\n     12|\n   null|\n+-------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 6-4. `filter`の利用例"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2bfa9c0b-ae89-43bb-a469-81fa13a9dfb0"}}},{"cell_type":"code","source":["(\n    df\n    .filter('int_col = \"string\"')\n    .show()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5746a37d-9c60-4412-a829-55ffe41467f8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+\n|int_col|\n+-------+\n| string|\n+-------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+\nint_col|\n+-------+\n string|\n+-------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 6-5. `alias`の利用例"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1bb4ab32-ab3f-4e72-826c-01757b101fdf"}}},{"cell_type":"code","source":["(\n    df\n    .alias('test')\n    .filter('test.int_col = \"string\"')\n    .show()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"921dd0a0-85a3-44d0-8779-1bb59a07d1ad"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+\n|int_col|\n+-------+\n| string|\n+-------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+\nint_col|\n+-------+\n string|\n+-------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 7. 制御フローによる PySpark 処理の実行"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6cb1c1cc-9b25-4684-8836-d7091de2f445"}}},{"cell_type":"markdown","source":["### 7-1. データフレームのデータ型を他のデータフレームに基づき変換"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"544134b8-cf9e-4f07-b2bc-13f4697685d4"}}},{"cell_type":"code","source":["schema_01 = '''\nstr_col string,\nint_col string,\ndate_col string\n'''\n\nschema_02 = '''\nstr_col string,\nint_col integer,\ndate_col date\n'''\n\nschema_03 = '''\nstr_col string,\nint_col integer\n'''\n\ndata_01 = [\n    [\n        'abc',\n        '123',\n        '2020-01-01'\n    ],\n    [\n        'def',\n        '456',\n        '2020-01-01'\n    ],\n]\n\ndf_1 = spark.createDataFrame(data_01,schema_01)\ndf_2 = spark.createDataFrame([],schema_02)\ndf_3 = spark.createDataFrame([],schema_03)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"033690a1-15e8-4986-b317-ef72ec84d5cb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# `df_2`のデータ型に、`df_1`のデータ型を変更\n\ndf_2_schema = df_2.schema.jsonValue()['fields']\n\ndf_2_col_names_and_data_types = {}\nfor df_2_col in df_2_schema:\n    df_2_col_names_and_data_types[df_2_col['name']] = df_2_col['type']\n\nprint(df_2_col_names_and_data_types)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6280e90f-dcc9-477c-b4aa-6a2a31a4ca2b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">{&#39;str_col&#39;: &#39;string&#39;, &#39;int_col&#39;: &#39;integer&#39;, &#39;date_col&#39;: &#39;date&#39;}\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{&#39;str_col&#39;: &#39;string&#39;, &#39;int_col&#39;: &#39;integer&#39;, &#39;date_col&#39;: &#39;date&#39;}\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 変換対象のデータフレームを準備\ntgt_df = df_1\ntgt_df.printSchema()\n\n\n# `df_1`のスキーマと`df_2`のスキーマが異なるカラムのデータ型を変更\ndf_1_schema = df_1.schema.jsonValue()['fields']\nfor df_1_col in df_1_schema:\n    df_1_col_name = df_1_col['name']\n    df_1_col_type = df_1_col['type']\n    df_2_col_type =  df_2_col_names_and_data_types.get(df_1_col_name, '')\n    if df_1_col_type != df_2_col_type:\n        if df_1_col_type == 'date':\n            tgt_df = tgt_df.withColumn(df_1_col_name, to_date(tgt_df[df_1_col_name], date_format))\n        else:\n            tgt_df = tgt_df.withColumn(df_1_col_name, tgt_df[df_1_col_name].cast(df_2_col_type))\n\ntgt_df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dfaab207-cf8c-46a7-a5d7-5e10a8362995"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- str_col: string (nullable = true)\n |-- int_col: string (nullable = true)\n |-- date_col: string (nullable = true)\n\nroot\n |-- str_col: string (nullable = true)\n |-- int_col: integer (nullable = true)\n |-- date_col: date (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- str_col: string (nullable = true)\n-- int_col: string (nullable = true)\n-- date_col: string (nullable = true)\n\nroot\n-- str_col: string (nullable = true)\n-- int_col: integer (nullable = true)\n-- date_col: date (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 7-2. カラムのリストから SELECT 文の カラム名を生成"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd2cd806-7f46-44cc-abe5-c0bd978d9029"}}},{"cell_type":"code","source":["# `df_3`でのみ保持している２つのカラムを取得する SELECT 文を生成\ndf_3_cols = df_3.columns\nprint(df_3_cols)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30c1ec3e-6a4d-4b9d-bed2-ce5398bed8c8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[&#39;str_col&#39;, &#39;int_col&#39;]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;str_col&#39;, &#39;int_col&#39;]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["select_cols = '\\n  ,'.join(df_3_cols)\nprint(select_cols)\nprint('\\n')\n\nsql = f'''\nSELECT\n  {select_cols}\n  FROM\n    TEST\n'''\nprint(sql)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68259a28-2abd-4552-834d-e8a1b9572414"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">str_col\n  ,int_col\n\n\n\nSELECT\n  str_col\n  ,int_col\n  FROM\n    TEST\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">str_col\n  ,int_col\n\n\n\nSELECT\n  str_col\n  ,int_col\n  FROM\n    TEST\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df_1.createOrReplaceTempView('test')\nspark.sql(sql).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ecf82007-180c-4d6e-8d3d-a6ee7d027311"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+-------+\n|str_col|int_col|\n+-------+-------+\n|    abc|    123|\n|    def|    456|\n+-------+-------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-------+\nstr_col|int_col|\n+-------+-------+\n    abc|    123|\n    def|    456|\n+-------+-------+\n\n</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"T10__L100__010","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":1640450761247783,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":1640450761247370}},"nbformat":4,"nbformat_minor":0}
