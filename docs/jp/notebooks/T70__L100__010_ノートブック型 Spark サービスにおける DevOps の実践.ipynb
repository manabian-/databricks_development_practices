{"cells":[{"cell_type":"markdown","source":["# ノートブック型 Spark サービスにおける DevOps の実践"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f2d4f70-c934-451b-84b0-a3fcc84b4db2"}}},{"cell_type":"markdown","source":["## ノートブック型 Spark サービスにおける DevOps の概要"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c893311b-e8d2-4e1a-874a-89d1a15225f0"}}},{"cell_type":"markdown","source":["ノートブック型 Spark サービスにおける CI/CD（Continuous Integration（継続的インテグレーション）/ Continuous Delivery（継続的デリバリー））の実施方法を説明する。CI/CD とは、コード開発から本番環境への統合までの品質の保証と期間の最小化を担保しつつ、変更点の統合を行いながらシステム運用を行うプロセスである。\n\nCI/CD は次のようなプロセスを実施することが多い。CI にてリリース可能なリソース（一般的には main ブランチのコード）の準備を行い、 CD にて他リソースと統合した本番相当の環境で検証を行ったうえで本番環境へリリースする。リソースの管理は Git で行い、 Git の運用ルールは Git-flow と呼ぶ。Git-flow はチームのスキルセットに応じて方針を定める必要があり、Git の機能を多様するような複雑な設計を行うと運用コストが高くなる可能性がある。継続的な開発と運用を行えるように、シンプルな設計が望ましい。CI/CD の処理群を CI/CD パイプラインと呼び、Github Actions や Azure Pipelines などのサービスで実行する。\n\n- CI\n  - Code\n    - Git レポジトリ（ main ブランチ）を準備。\n    - main ブランチから派生した開発用レポジトリー（フォークしたレポジトリ、あるいは、ブランチ）を準備。\n    - プログラムコードとテストコードを開発。\n    - 開発用レポジトリーへ変更内容をコミット。\n    - Main ブランチへ変更要求（Pull Requet）を実施。\n  - Build\n    - main ブランチをベースとして PR の変更内容を反映したコードを生成。\n    - ライブラリ等のビルド。\n    - テスト用環境の準備。\n    - テストを実行。\n    - リリースの実施有無を判断。\n  - Release\n    - 環境へデプロイ可能なリソースを生成。\n- CD\n  - Deploy\n    - 環境へリソースを配布。\n  - Test\n    - 環境にてテスト実行（主に他システムとの連携テストやE2Eテストを実施）。\n  - Opeate\n    - システムの運用。\n\nCode や Build では、開発支援の観点、及び、コードの品質管理の観点により、次のような項目の実施が求められる。\n\n1. Linter ツールによる静的解析\n1. Formatter ツールによるコード整形\n1. テスト結果の取得、及び、テスト結果の継続的な管理\n1. テストのコードカバレッジの取得、及び、テストのコードカバレッジの継続的な管理"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f36442fa-6926-45e0-8344-96940a1eb9cd"}}},{"cell_type":"markdown","source":["## Databricks での開発"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73a2352a-03a4-43cd-8715-de7cbf76721e"}}},{"cell_type":"markdown","source":["### Databricks の開発方針"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c18546c8-b5ef-435d-92d4-ca92e52dac31"}}},{"cell_type":"markdown","source":["Databricks Repos のファイル管理仕様、Databricks Workspace 上ではノートブックであるが Git 上では Python ファイルとして管理される仕様、を考慮して、次のような開発指針とする。\n\n- 共通機能を保持したノートブックを`%run`により呼び出して開発を行うこと\n- テストケースを unittest で記載し、ノートブック上で正常終了することを確認すること\n- ローカル環境での動作を考慮したコードを記載すること\n\nローカル環境での動作を行う目的は、ノートブック型環境で実施できないことを補うためである。ノートブック型環境には、Visual Studio Code (VS Code) などのローカル環境と比較すると、次のような特徴がある。ノートブック型環境の短所を克服するために、ノートブック型環境とローカル環境の併用がおすすめ。\n\n- Pros\n  1. 実行ノートブックにおけるコードと結果を合わせて保存できること\n  1. 開発環境の準備が最小限となり、実行環境とコードの共有ができること\n  1. 処理をセルごとに分割することで処理フローの解釈が容易となること\n- Cons\n  1. デバック実行が不可\n  1. Linter（静的解析）ツールの利用不可\n  1. Formatter （コード整形）ツールの利用不可\n  1. テストのコードカバレッジの取得不可"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53e32e72-2c32-46c3-a52d-b86ad776b244"}}},{"cell_type":"markdown","source":["### ローカル環境（Visual Studio Code） 上での開発"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec2912bb-1b67-47a3-9d70-e45bd1bb8da0"}}},{"cell_type":"markdown","source":["#### ローカル環境での開発方針"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"83e0af21-aa54-451f-8242-3afa11646810"}}},{"cell_type":"markdown","source":["ローカル環境（Visual Studio Code）の開発環境の構築方法として下記表の方法が考えられるが、ノートブック型環境の短所を補うための補足的な利用であるため、1 の方法を採用。CI/CD パイプラインの実行時 Databricks 上で実施するため、Databricks に近い環境を選択。\n\n| #    | 方法                                      | Pros                                                         | Cons                                                         |\n| ---- | ----------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| 1    | Databricks Connect により開発する方法     | - VS Code 上でテスト実施でき、コードカバレッジも取得可能     | - 一部機能を利用することができず、新機能の開発の計画がない                           |\n| 2    | dbx by Databricks Labs により開発する方法 | - Databricks Workspace ローカル上のコードを、ローカル環境と同期が可能 | - ベンダーのサポートがない<br />- コードカバレッジを取得する方法を確立できていない |\n| 3    | ローカル環境の Spark 環境にて開発する方法 | - VS Code 上でテスト実施でき、コードカバレッジも取得可能     | - Spark プロバイダー固有機能のテストを実施できない           |\n\nDatabricks Connect 環境の構築方法としては下記表の方法が考えられるが、１の方法を採用。ローカル環境の OS が Windows の場合には、Databricks で利用されている OS との仕様差異を最小限とするために、Databricks Runtime と同じバージョンの OS (Databricks Runtime 10.4 LTS を利用する場合には Ubuntu 20.04 LTS) を用いることを推奨。\n\n| #    | 方法                                                         | Pros                                             | Cons                                                         |\n| ---- | ------------------------------------------------------------ | ------------------------------------------------ | ------------------------------------------------------------ |\n| 1    | Conda （無償：Miniconda、有償：Anaconda）を用いて構築する方法 | - 初期環境の統一ができ、環境を柔軟に切り替え可能 | - 環境の統一が困難                                           |\n| 2    | Poetry を用いて構築する方法                                  | - 環境の統一が容易                               | - Databricks Connect のバージョンを切り替えることができない |\n| 3    | コンテナ（無償：Rancher Desktop、有償：Docker Desktop）を用いて構築するする方法             | - 初期環境の統一ができ、環境を柔軟に切り替え可能  | - コンテナーに関する知識が必要 |\n| 4    | Python 仮想環境を用いずに直接構築する方法             | - 特になし                                       | - Databricks Connect のバージョンを切り替えることができない |\n\nVisual Studio Code の設定値を開発メンバーで共有するために、次のファイルを`.vscode`ディレクトリに配置。\n\n| #    | ファイル名      | 概要                                                      |\n| ---- | --------------- | --------------------------------------------------------- |\n| 1    | settings.json   | Visual Studio Code の設定を保持。                         |\n| 2    | extensions.json | Visual Studio Code に追加推奨の拡張機能のリストを保持。   |\n| 3    | launch.json     | Visual Studio Code にてデバッグ実行する際のの設定を保持。 |\n\nPython の Linter と Formatter として次のライブラリを利用。\n\n| #    | 分類      | ライブラリ名 | 目的                               |\n| ---- | --------- | ------------ | ---------------------------------- |\n| 1    | Formatter | black        | コードに対するフォーマットツール。             |\n| 2    | Formatter | isort        | import文に対するフォーマットツール。           |\n| 3    | Linter    | flake8       | コードに対する静的解析ツール。     |\n| 4    | Linter    | pydocstyle   | Docstring に対する静的解析ツール。 |\n| 5    | Linter    | mypy         | 型に対する静的解析ツール。         |\n\nライブラリの設定値は、次のファイルにて設定。\n\n| #    | ファイル名     | 概要                                                         |\n| ---- | -------------- | ------------------------------------------------------------ |\n| 1    | pyproject.toml | 基本的な設定値を設定。                                       |\n| 2    | .vscode/settings.json  | ローカル環境でのみ有効な設定値を設定。テスト実行時の並列数の設定など。 |\n| 3    | .vscode/launch.json    | デバッグ時にのみ有効な設定値を設定。カバレッジレポートを取得しない設定など。 |\n\n\nConda 環境情報を`environment.yml`に記述しており、Databricks Connect に対応した Python バージョンをベースに`requirements.txt`を pip install する構成。`requirements.txt`では、Databricks Connect や Linter などの開発時に必要なライブラリだけでなく、Databricks Runtime にインストールされている多くのライブラリを保持。\n\n```requirements.txt\n# Formatting libraries\nblack\nisort\n\n# Linter libraries\nflake8\npydocstyle\nmypy\n\n# test libraries\npytest\ncoverage\npytest-cov\npytest-xdist\n\n# Sphinx Libraries\nsphinx\nsphinx-markdown-builder\n\n# databricks Connect\ndatabricks-connect==10.4.*\n```\n\nDatabricks Runtime インストールされているライブラリのバージョンを取得するために、次のような手順で `requirements.txt` を作成している。Conda 環境構築時にエラーとなったライブラリーを除外。\n\n1. `%pip freeze`をノートブック上で実行することで、クラスターにインストールされているライブラリ一覧を取得\n2. `requirements.txt`に、１の結果を貼り付け、不要なライブラリを削除\n3. `conda env create -f environment.yml`により環境が構築できることを確認\n4. `.environments` ディレクトリ下に Databricks Connect のバージョンごとの環境情報を保存"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38a1d7e8-4b8b-41f1-883c-d4dd77dde7b2"}}},{"cell_type":"code","source":["%pip freeze"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3ff1770-82ff-4df6-98c6-a49cd508338c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">appdirs==1.4.4\nargon2-cffi==20.1.0\nasync-generator==1.10\nattrs==20.3.0\nbackcall==0.2.0\nbidict==0.21.4\nbleach==3.3.0\nboto3==1.16.7\nbotocore==1.19.7\ncertifi==2020.12.5\ncffi==1.14.5\nchardet==4.0.0\ncoverage==6.5.0\ncycler==0.10.0\nCython==0.29.23\ndbus-python==1.2.16\ndecorator==5.0.6\ndefusedxml==0.7.1\ndistlib==0.3.4\ndistro==1.4.0\ndistro-info===0.23ubuntu1\nentrypoints==0.3\nfacets-overview==1.0.0\nfilelock==3.6.0\nidna==2.10\niniconfig==1.1.1\nipykernel==5.3.4\nipython==7.22.0\nipython-genutils==0.2.0\nipywidgets==7.6.3\njedi==0.17.2\nJinja2==2.11.3\njmespath==0.10.0\njoblib==1.0.1\njsonschema==3.2.0\njupyter-client==6.1.12\njupyter-core==4.7.1\njupyterlab-pygments==0.1.2\njupyterlab-widgets==1.0.0\nkiwisolver==1.3.1\nkoalas==1.8.2\nMarkupSafe==2.0.1\nmatplotlib==3.4.2\nmistune==0.8.4\nnbclient==0.5.3\nnbconvert==6.0.7\nnbformat==5.1.3\nnest-asyncio==1.5.1\nnotebook==6.3.0\nnumpy==1.20.1\npackaging==20.9\npandas==1.2.4\npandocfilters==1.4.3\nparso==0.7.0\npatsy==0.5.1\npexpect==4.8.0\npickleshare==0.7.5\nPillow==8.2.0\nplotly==5.5.0\npluggy==1.0.0\nprometheus-client==0.10.1\nprompt-toolkit==3.0.17\nprotobuf==3.17.2\npsycopg2==2.8.5\nptyprocess==0.7.0\npy==1.11.0\npyarrow==4.0.0\npycparser==2.20\nPygments==2.8.1\nPyGObject==3.36.0\npyparsing==2.4.7\npyrsistent==0.17.3\npytest==7.1.3\npytest-cov==4.0.0\npython-apt==2.0.0+ubuntu0.20.4.8\npython-dateutil==2.8.1\npython-engineio==4.3.0\npython-socketio==5.4.1\npytz==2020.5\npyzmq==20.0.0\nrequests==2.25.1\nrequests-unixsocket==0.2.0\ns3transfer==0.3.7\nscikit-learn==0.24.1\nscipy==1.6.2\nseaborn==0.11.1\nSend2Trash==1.5.0\nsix==1.15.0\nssh-import-id==5.10\nstatsmodels==0.12.2\ntenacity==8.0.1\nterminado==0.9.4\ntestpath==0.4.4\nthreadpoolctl==2.1.0\ntomli==2.0.1\ntornado==6.1\ntraitlets==5.0.5\nunattended-upgrades==0.1\nurllib3==1.25.11\nvirtualenv==20.4.1\nwcwidth==0.2.5\nwebencodings==0.5.1\nwidgetsnbextension==3.5.1\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">appdirs==1.4.4\nargon2-cffi==20.1.0\nasync-generator==1.10\nattrs==20.3.0\nbackcall==0.2.0\nbidict==0.21.4\nbleach==3.3.0\nboto3==1.16.7\nbotocore==1.19.7\ncertifi==2020.12.5\ncffi==1.14.5\nchardet==4.0.0\ncoverage==6.5.0\ncycler==0.10.0\nCython==0.29.23\ndbus-python==1.2.16\ndecorator==5.0.6\ndefusedxml==0.7.1\ndistlib==0.3.4\ndistro==1.4.0\ndistro-info===0.23ubuntu1\nentrypoints==0.3\nfacets-overview==1.0.0\nfilelock==3.6.0\nidna==2.10\niniconfig==1.1.1\nipykernel==5.3.4\nipython==7.22.0\nipython-genutils==0.2.0\nipywidgets==7.6.3\njedi==0.17.2\nJinja2==2.11.3\njmespath==0.10.0\njoblib==1.0.1\njsonschema==3.2.0\njupyter-client==6.1.12\njupyter-core==4.7.1\njupyterlab-pygments==0.1.2\njupyterlab-widgets==1.0.0\nkiwisolver==1.3.1\nkoalas==1.8.2\nMarkupSafe==2.0.1\nmatplotlib==3.4.2\nmistune==0.8.4\nnbclient==0.5.3\nnbconvert==6.0.7\nnbformat==5.1.3\nnest-asyncio==1.5.1\nnotebook==6.3.0\nnumpy==1.20.1\npackaging==20.9\npandas==1.2.4\npandocfilters==1.4.3\nparso==0.7.0\npatsy==0.5.1\npexpect==4.8.0\npickleshare==0.7.5\nPillow==8.2.0\nplotly==5.5.0\npluggy==1.0.0\nprometheus-client==0.10.1\nprompt-toolkit==3.0.17\nprotobuf==3.17.2\npsycopg2==2.8.5\nptyprocess==0.7.0\npy==1.11.0\npyarrow==4.0.0\npycparser==2.20\nPygments==2.8.1\nPyGObject==3.36.0\npyparsing==2.4.7\npyrsistent==0.17.3\npytest==7.1.3\npytest-cov==4.0.0\npython-apt==2.0.0+ubuntu0.20.4.8\npython-dateutil==2.8.1\npython-engineio==4.3.0\npython-socketio==5.4.1\npytz==2020.5\npyzmq==20.0.0\nrequests==2.25.1\nrequests-unixsocket==0.2.0\ns3transfer==0.3.7\nscikit-learn==0.24.1\nscipy==1.6.2\nseaborn==0.11.1\nSend2Trash==1.5.0\nsix==1.15.0\nssh-import-id==5.10\nstatsmodels==0.12.2\ntenacity==8.0.1\nterminado==0.9.4\ntestpath==0.4.4\nthreadpoolctl==2.1.0\ntomli==2.0.1\ntornado==6.1\ntraitlets==5.0.5\nunattended-upgrades==0.1\nurllib3==1.25.11\nvirtualenv==20.4.1\nwcwidth==0.2.5\nwebencodings==0.5.1\nwidgetsnbextension==3.5.1\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### ローカル環境の構築"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2617038-73c9-45b3-8a03-a289e9f870ca"}}},{"cell_type":"markdown","source":["ローカル環境として、Windows Subsystem for Linux (WSL)上に環境を構築する手順を実施。\n\n1. WSL 上の Ubuntu 20.04 環境をエクスポートして別名でインポート\n1. miniconda をインストール\n1. conda env を作成\n1. Databricks  への接続設定\n\n**0.事前準備**\n\n事前準備として、以下のインストールを事前に実施。\n\n- [Windows Terminal](https://apps.microsoft.com/store/detail/9N0DX20HK701?hl=ja-jp&gl=JP)\n- [Visual Studio Code](https://code.visualstudio.com/download)\n  - [Remote Development](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack)\n- [Windows Subsystem for Linux](https://learn.microsoft.com/ja-jp/windows/wsl/install)\n  - [Ubuntu 20.04 - Microsoft Store アプリ](https://apps.microsoft.com/store/detail/ubuntu-2004/9N6SVWS3RX71?hl=ja-jp&gl=jp)\n\n**1. WSL 上の Ubuntu 20.04 環境をエクスポートして別名でインポート**\n\nWindows Terminal にて powershell を起動後、以下のコマンドを実行\n\n```powershell\n# Set wsl distro name\n$src_wsl_distro_name = \"Ubuntu-20.04\"\n$tgt_wsl_distro_name = \"databricks-connect\"\n```\n\n```powershell\n# Update wsl version to 2\nwsl --set-version $src_wsl_distro_name 2\n```\n\n```powershell\n$wsl_file_name = $src_wsl_distro_name + \".tar\"\nwsl --export $src_wsl_distro_name $wsl_file_name\n```\n\n\n```powershell\n# Import wsl distro_name\nwsl --import $tgt_wsl_distro_name $tgt_wsl_distro_name $wsl_file_name\n```\n\n```powershell\n# Check wsl dstro list\nwsl -l\n\n# login to wsl distro\nwsl -d $tgt_wsl_distro_name -u root\n```\n\n`default_user_name` 変数にて、ubuntu で作成したユーザー名をセットして、以下のコマンドを実行。\n\n```bash\n# Set default user name\ndefault_user_name=wsl_user\n\ncat << EOF > /etc/wsl.conf\n[user]\ndefault=$default_user_name\nEOF\n\n# chekc wsl.conf\ncat /etc/wsl.conf\n```\n\n```bash\n# logout\nexit\n```\n\n```powershell\n# Delete distro file\nRemove-Item $wsl_file_name\n```\n\n```powershell\n# reboot and re-login\nwsl -t $tgt_wsl_distro_name\nwsl -d $tgt_wsl_distro_name\n```\n\n```bash\nsudo apt update && sudo apt upgrade -y\n```\n\n```bash\n# install java required by databricks-connect\nsudo apt install openjdk-8-jre -y\n```\n\n```bash\n# check java version\njava -version\n```\n\n**2. miniconda をインストール**\n\nWindows Terminal を再起動後、 1 の手順で作成した環境のコマンドラインシェルを起動後、次のコマンドを実行。\n\n```bash\n# download miniconda\ncd\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n```\n\n```bash\n# install miniconda\nbash Miniconda3-latest-Linux-x86_64.sh\n```\n\n```bash\n# delete miniconda sh file\nrm Miniconda3-latest-Linux-x86_64.sh\n```\n\n```bash\n# create directory for git\nmkdir source\n```\n\n**3. conda env を作成**\n\nVS Code から、1 の手順で作成した WSL 環境に接続後、次の手順により、Git Provider(Azure Repos)からコードをクローン。\n\n1. Azure DevOps にてクローンする レポジトリーの `Repos` -> `Files`ページに接続後、`clone`を選択\n1. `IDE`にある`Clone in VS Code`を選択し、VS Code に画面に切り替わることを確認\n1. `Choose a folder to clone` プロンプトにて、 2 の手順で作成した `source` を選択\n1. Azure DevOps に戻り、`Generate Git Credentials`を選択し、`Password` の値を取得\n1. VS Code にて、`Password` を入力後、コードがクローンされることを確認\n1. クローンしたディレクトリを開き、Azure DevOps 上のディレクトリ構成となっていることを確認\n1. `Do you want to install the recommended extensions for this repository` プロンプトが表示された場合には、`install` を選択\n\nクローン完了後、VS Code のターミナルにて、次のコマンドを実行。`databricks-connect` のファイルサイズが大きいことでタイムアウトが発生する場合には、`pip istall` により`databricks-connect`を個別にインストールし、`pip install -r requirements.txt`によりその他のライブラリをインストールする手順に変更する。\n\n```bash\n# create conda envrioment for databricks connect\nconda env create -f environment.yml\n```\n\n```bash\n# check conda env\nconda env list\n```\n\n**4. Databricks  への接続設定**\n\nVS Code のターミナルにて、次のコマンドを実行。設定値については、[ドキュメント](https://learn.microsoft.com/ja-jp/azure/databricks/dev-tools/databricks-connect#step-2-configure-connection-properties)を参照。通常の手順では、`.databricks-connect`にて接続情報が格納される。\n\n```bash\n# connect to databricks connect conda env\nconda activate dbconnect_10_4\n\n# config databricks connect\ndatabricks-connect configure\n```\n\n```bash\n# test connect to databricks cluster\ndatabricks-connect test\n```\nDatabricks Connect の接続確認後、次の手順を実施。\n\n1. VS Code にて、`Ctrl+Shift+P` -> `Python:Select Interpreter` を選択後、作成した Conda 環境（`dbconnect_10_4`）を選択。\n1. タスクバーにある VS Code アイコンを右クリックし、現在開いているディレクトリ（例：`spark_engineering_template[wsl: databrikcs-connect]``）の`一覧にピン留めする` を選択"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a921fbc4-dbe3-4cb7-88b5-c4b7c263416b"}}},{"cell_type":"markdown","source":["## Azure DevOps (Pipelines) による CI/CD パイプライン"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8723e1cb-474c-4169-9303-ab46e39b2727"}}},{"cell_type":"markdown","source":["### Azure Pipelines 概要"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4fa4315f-f04f-42a3-825f-f49f24e8f0e7"}}},{"cell_type":"markdown","source":["Azure Pipelines とは、Azure DevOps の１機能であり、 CI/CD（Continuous Integration（継続的インテグレーション）/ Continuous Delivery（継続的デリバリー））を自動化させる pipeline を構築できるサービスである。pipeline は、stage -> job -> task の階層構造となってり、Trigger により実行され、 Stage により論理的境界線を定義し、 job にエージェントを紐づけて task で指定した処理を行う。\n\n-  [Azure Pipelines New User Guide - Key concepts - Azure Pipelines | Microsoft Learn](https://learn.microsoft.com/ja-jp/azure/devops/pipelines/get-started/key-pipelines-concepts?view=azure-devops)\n\nAzure Pipelines では、エージェントの並列実行数が主な課金対象であり、複数人での開発を行う際には無償で並列数に制限なく利用できるエージェントレスジョブを利用するなどのエージェントの利用方法が重要となる。処理が完了するまでの待機処理を、通常のジョブで実行した場合にはエージェントを占有してしまうが、エージェントレスジョブで実行することで他パイプライン（他ジョブ）がエージェントを利用することができる。\n\n- [Azure Pipelines エージェント - Azure Pipelines | Microsoft Learn](https://learn.microsoft.com/ja-jp/azure/devops/pipelines/agents/agents?view=azure-devops&tabs=browser)\n- [エージェントレス ジョブでサポートされているタスク - Azure Pipelines | Microsoft Learn](https://learn.microsoft.com/ja-jp/azure/devops/pipelines/process/phases?view=azure-devops-2020&tabs=yaml#agentless-tasks)\n\nAzure Pipelines では、YAML 形式で記述を行う。YAMLで記述できる項目は、Azure DevOps のドキュメントにて整理されており、タスク等の拡張機能をインストールすることも可能である。[Command Line task](https://learn.microsoft.com/ja-jp/azure/devops/pipelines/tasks/utility/command-line?view=azure-devops&tabs=yaml) では`script`と記述できるなど、一部のタスクではシンタックスを利用することもできる。以前から Azure DevOps （Visual Studio Team Services） で利用できたクラシックパイプラインという開発方法もあるが、本記事では利用対象としない。クラシックパイプラインの機能をベースに解説されている記事もあるため、どちらの開発方法の記事であるかを確認する必要がある。\n\n-   [YAML schema reference | Microsoft Learn](https://learn.microsoft.com/ja-jp/azure/devops/pipelines/yaml-schema/?view=azure-pipelines&viewFallbackFrom=azure-devops)\n-   [Catalog of the built-in tasks for build-release and Azure Pipelines & TFS - Azure Pipelines | Microsoft Learn](https://learn.microsoft.com/ja-jp/azure/devops/pipelines/tasks/?view=azure-devops)\n-   [Azure Pipelines Extensions - Azure DevOps Services (visualstudio.com)](https://marketplace.visualstudio.com/search?target=AzureDevOps&category=Azure Pipelines&sortBy=Installs)\n\nstages 、 jobs 、 tasks 、variables を別の YAML ファイルで定義し、テンプレートとして呼び出すことも可能。\n\n- [Templates - Azure Pipelines | Microsoft Learn](https://learn.microsoft.com/ja-jp/azure/devops/pipelines/process/templates?view=azure-devops)\n  -   [stages.template definition | Microsoft Learn](https://learn.microsoft.com/ja-jp/azure/devops/pipelines/yaml-schema/stages-template?view=azure-pipelines)\n  -   [jobs.template definition | Microsoft Learn](https://learn.microsoft.com/ja-jp/azure/devops/pipelines/yaml-schema/jobs-template?view=azure-pipelines)\n  -   [steps.template definition | Microsoft Learn](https://learn.microsoft.com/ja-jp/azure/devops/pipelines/yaml-schema/steps-template?view=azure-pipelines)\n  -   [variables.template definition | Microsoft Learn](https://learn.microsoft.com/ja-jp/azure/devops/pipelines/yaml-schema/variables-template?view=azure-pipelines)\n\n\n機微な情報をパイプラインで利用する際には、シークレット変数とする方法、および、Azure Key Vault を利用する方法が推奨されており、YAML 上に直接記述するべきではない。\n\n-   [シークレット変数を設定する - Azure Pipelines | Microsoft Learn](https://learn.microsoft.com/ja-jp/azure/devops/pipelines/process/set-secret-variables?view=azure-devops&tabs=yaml%2Cbash)\n-   [Azure Pipelines で Azure Key Vault シークレットを使用する - Azure Pipelines | Microsoft Learn](https://learn.microsoft.com/ja-jp/azure/devops/pipelines/release/azure-key-vault?view=azure-devops&tabs=yaml)\n\n\nパイプライン実行の承認（Approvals）や同時実行の制御（Exclusive lock）も実施できる。\n\n-   [Approvals - Azure Pipelines | Microsoft Learn](https://learn.microsoft.com/en-us/azure/devops/pipelines/process/approvals?view=azure-devops&preserve-view=true&tabs=check-pass#approvals)\n-   [Exclusive lock - Azure Pipelines | Microsoft Learn](https://learn.microsoft.com/en-us/azure/devops/pipelines/process/approvals?view=azure-devops&preserve-view=true&tabs=check-pass#exclusive-lock)\n\n\nジョブが異なる場合には異なるエージェント環境となるため、変数の受け渡しを行う際には次の記事で記載されているような記述を行う。`dependsOn`により変数取得元のジョブ間に依存関係を明示する必要がある。\n\n- [Support for output variables - Azure Pipelines | Microsoft Learn](https://learn.microsoft.com/en-us/azure/devops/pipelines/process/deployment-jobs?view=azure-devops#support-for-output-variables)\n\n\nビルド番号などの定義済み変数については、次のドキュメントに記載されている。\n\n- [実行 (ビルド) 番号 - Azure Pipelines | Microsoft Learn](https://learn.microsoft.com/ja-jp/azure/devops/pipelines/process/run-number?view=azure-devops&tabs=yaml)\n\nAzure pipelins の実装方法を検討する際には、次のドキュメントの方法を試すことが参考になる。\n\n- [Python アプリのビルドとテスト - Azure Pipelines | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/devops/pipelines/ecosystems/python?view=azure-devops)\n- [Azure DevOps を使用した Azure Databricks での継続的インテグレーションとデリバリー - Azure Databricks | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/databricks/dev-tools/ci-cd/ci-cd-azure-devops)\n\nPython アプリのビルドとテストを実施する際には、次のようなシンプルなパイプラインを実行できる。\n\n```yaml\ntrigger: none\npr: none\n\njobs:\n- job:\n  displayName: execute and publish test\n  pool:\n    vmImage: 'ubuntu-latest'\n  steps:\n  - task: UsePythonVersion@0\n    inputs:\n      versionSpec: '3.7'\n  - script: python -m pip install --upgrade pip setuptools wheel\n    displayName: 'Install tools'\n  - script: pip install -r requirements.txt\n    displayName: 'Install requirements'\n  - script: |\n      pip install pytest pytest-azurepipelines\n      pip install pytest-cov\n      pytest --doctest-modules --junitxml=junit/test-results.xml --cov=. --cov-report=xml\n    displayName: 'pytest'\n  - task: PublishTestResults@2\n    condition: succeededOrFailed()\n    inputs:\n      testResultsFiles: '**/test-*.xml'\n      testRunTitle: 'Publish test results for Python $(python.version)'\n  - task: PublishCodeCoverageResults@1\n    inputs:\n      codeCoverageTool: Cobertura\n      summaryFileLocation: '$(System.DefaultWorkingDirectory)/**/coverage.xml'\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20db830d-1bca-4221-8470-b2331f6b5c6f"}}},{"cell_type":"markdown","source":["### Databricks における CI/CD パイプラインの構築"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8ff5825-2750-457f-9584-2f7f1d26d064"}}},{"cell_type":"markdown","source":["#### Azure Pipelines から Databricks を操作する方法"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ab695644-0271-4334-adc5-fa3881d4fa3a"}}},{"cell_type":"markdown","source":["Azure Pipelines から Databricks を操作するには、次の方法を実施する。\n\n1. CmdLineタスクにより Databricks REST API を実行する方法\n1. CmdLineタスクにより Databricks CLI を実行する方法\n\n```yaml\n# CmdLineタスクにより Databricks REST API を実行する方法\njobs:\n  - job: Databrics_REST_API_by_CmdLine\n    steps:\n    - task: CmdLine@2\n      displayName: Execute Databricks Workflow\n      inputs:\n        script: |\n          results=$(curl -X POST $(DATABRICKS_WORKSPACE_URL)/api/2.1/jobs/run-now \\\n          -H 'Content-Type: application/json' \\\n          -H 'Authorization: Bearer $(DATABRICKS_TOKEN)' \\\n          -d '{\"job_id\": \"$(DATABRICKS_JOB_ID)\"}' \\\n          )\n          echo $results\n```\n\n```yaml\n# CmdLineタスクにより Databricks CLI を実行する方法\njobs:\n  - job: Databrics_CLI\n    steps:\n    - task: UsePythonVersion@0\n      displayName: 'Use Python 3.8'\n    - task: CmdLine@2\n      inputs:\n        script: |\n          pip install databricks-cli -q\n          databricks jobs run-now --job-id $(DATABRICKS_JOB_ID)\n      env:\n        DATABRICKS_HOST: $(DATABRICKS_WORKSPACE_URL)\n        DATABRICKS_TOKEN: $(DATABRICKS_TOKEN)\n```\n\nAPI については、次のドキュメントが参考になる。\n\n- [Databricks CLI - Azure Databricks | Microsoft Learn](https://learn.microsoft.com/ja-jp/azure/databricks/dev-tools/cli/)\n- [REST API (latest) | Databricks on AWS](https://docs.databricks.com/dev-tools/api/latest/index.html)\n\n\nCI / CD 時には、サービスプリンシパル、あるいは、システムアカウントを利用することが推奨。\n\n>  Azure AD サービス プリンシパルとその Azure AD トークンを使用するか、ワークスペース ユーザー用の Azure Databricks 個人用アクセス トークンを使用して、CI/CD プラットフォームに Azure Databricks リソースへのアクセス権を付与することをお勧めします。\n\n引用元：[CI/CD のサービス プリンシパル - Azure Databricks | Microsoft Learn](https://learn.microsoft.com/ja-jp/azure/databricks/dev-tools/ci-cd/ci-cd-sp)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a60e2ecd-21b0-4147-aada-02005878bbbb"}}},{"cell_type":"markdown","source":["#### Databricks における CI パイプラインの構築"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"968aaea4-56af-430a-a27d-e7ee5c4cc48e"}}},{"cell_type":"markdown","source":["CI パイプラインをとしては、次のような処理を実行している。`Python スクリプトでの単体テストの実施` にて、複数の Databricks ランタイムでテストを実施している。\n\n1. ノートブックでの単体テストの実施\n    - Databricks Repos を作成\n    - Databricks Workflows(Notbook Type) でのテストの実施とテスト結果の発行\n      - Databricks Workflows(Notbook Type) の作成と実行\n      - Databricks Workflows 処理の完了確認\n      - テスト結果（JUnit 形式の XML ファイル）を Azure Pipelines に発行\n1. Python スクリプトでの単体テストの実施\n    - コードを DBFS にコピー\n    - Databricks Workflows(Python script type)でのテストの実施とテスト結果の発行\n      - Databricks Workflows(Python script type)の作成と実行\n      - Databricks Workflows 処理の完了確認\n      - テスト結果（JUnit 形式の XML ファイル）を Azure Pipelines に発行\n1. 単体テストのコードカバレッジの発行\n    - コードカバレッジ結果の統合と発行\n      - DBFS からコードカバレッジ結果（.coverage）をコピー\n      - コードカバレッジ結果の統合（ combine ）\n      - コードカバレッジ結果を Cobertura 形式の XML ファイルへ変換\n      - コードカバレッジ結果を発行\n1. 事後処理\n    - DBFS 上のコード等を削除\n    - Databricks Repos 上のディレクトリを削除\n\n\n想定のトリガー\n\n1. Pull Request 作成がされたとき\n2. main レポジトリー、あるいは、release/* レポジトリーに対して Merge 処理がされたとき"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b38b1e5-acca-4807-a2e2-6a13aafbbf6d"}}},{"cell_type":"markdown","source":["#### Databricks における CD パイプラインの構築"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7e742ea-b216-436b-8424-fbb50339e78c"}}},{"cell_type":"markdown","source":["CD パイプラインとしては、状況に合わせて統合テスト等を含めるなどの対応を行う。\n\nDatabricks Repos 、あるいは、 Databricks Workspace にコードを配置する際には、次の方法を実施。\n\n1. Databricks Repos を用いる場合には、[Repos API](https://docs.databricks.com/dev-tools/api/latest/repos.html) を利用\n1. Databricks Workspace を用いる場合には、[Databricks CLI](https://learn.microsoft.com/ja-jp/azure/databricks/dev-tools/cli/) を利用\n\n\n次のトリガーを想定。誤ってデプロイが実施されないように、パイプライン実行の承認の設定が推奨。\n\n1. Manual トリガー\n2. スケジュール設定"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"918e9926-cfad-413b-8f8b-cf68f97aef8b"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"T70__L100__010","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1640450761248085}},"nbformat":4,"nbformat_minor":0}
