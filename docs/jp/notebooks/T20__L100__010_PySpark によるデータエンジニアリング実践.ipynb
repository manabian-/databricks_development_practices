{"cells":[{"cell_type":"markdown","source":["# PySpark によるデータエンジニアリング実践"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0fc0fe6-0940-4f85-94d3-7ff8265d2f7c"}}},{"cell_type":"markdown","source":["PySparkにてデータエンジニアリングを実施する際に知っておくべき次のテーマを説明する。\n\n1. ACID トランザクションの保証されているデータフォーマットの利用\n1. レイテンシーに応じたデータ処理方法の選択\n1. メダリオンアーキテクチャによるデータエンジニアリング\n1. Spark によるデータエンジニアリングに利用すべきプログラミング言語とライブラリー\n1. パフォーマン最適化\n1. データエンジニアリングにおける Spark 関連\n1. 処理の共通化"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e3c1f82-ab10-489b-a65a-19ee5f57ae28"}}},{"cell_type":"markdown","source":["## 1. ACID トランザクションが保証されるデータフォーマットの利用"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e8fbc94-1a45-4bfa-ab3a-87351aa2be74"}}},{"cell_type":"markdown","source":["データレイクでは、ソースから連携される CSV などのファイルをそのまま蓄積するものという固定概念があるが、構造データと半構造データを ACID トランザクションをサポートしているファイル形式（実際には複数種のファイルやディレクトリで構成されていることからファイルレイヤーと記載されていることもあり）として保存する方法が普及している。\n\nOSSとして次のファイル形式がよく利用さている。Databricks では、基本的には Delta Lake 形式を用いることが推奨事項である。\n　\n-   [Delta Lake](https://delta.io/)\n-   [Apache Iceberg](https://iceberg.apache.org/)\n-   [Apache Hudi](https://hudi.apache.org/)\n\n\nDelta Lake 形式では、差分連携時に Merge 処理（ Upsert 処理）が最適な方法として提供されているなどデータエンジニアリングのプログラムがシンプルとなる。従来の DWH 製品では更新処理が性能ボトルネックとなることからデータ書き込みプログラムが複雑となることがあった。\n\n- [Delta Lake Tutorial: How to Easily Delete, Update, and Merge Using DML - The Databricks Blog](https://databricks.com/blog/2020/09/29/diving-into-delta-lake-dml-internals-update-delete-merge.html)\n\n分離レベルが`WriteSerializable`であるため、従来の DWH 製品で実施される参照ロックを行われないこともあり、利用者が少ない夜間などのバッチ処理としてデータの書き込みを実施する必要もない。\n\n- [分離レベル - Azure Databricks | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/databricks/delta/optimizations/isolation-level)\n- [コンカレンシー制御 - Azure Databricks | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/databricks/delta/concurrency-control)\n\nスキーマ適用（ schema enforcement ）とスキーマ展開（ schema evolution ）により、プログラムの拡張性と信頼性のバランスをとることができる。\n\n- [Schema Evolution & Enforcement on Delta Lake - Databricks](https://databricks.com/jp/blog/2019/09/24/diving-into-delta-lake-schema-enforcement-evolution.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"799cfa82-b9b6-4f23-ab70-41ba2b497faa"}}},{"cell_type":"markdown","source":["## 2. レイテンシーに応じたデータ処理方法の選択"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"36cfcf14-aa55-4aae-8429-2ba2d847843c"}}},{"cell_type":"markdown","source":["データの生成からデータが利用可能になるまでの時間差（レイテンシー）要件に応じて、データ処理方法を選択する必要がある。データ処理方法に合わせて、ワークフローとして管理する方法の検討も行う。\n\n| #    | レイテンシーに応じた処理方法                     | 実装例                                           |\n| ---- | ---------------------------- | ------------------------------------------------------------ |\n| 1    | バッチ                       | 1-1. スケジュールトリガーによるSparkデータフレーム処理<br/>1-2. Delta live tableのトリガーパイプラインによる処理 |\n| 2    | 準リアルタイムとイベント駆動 | 2-1. ファイル到着イベントトリガーによる実行               |\n| 3    | リアルタイム               | 3-1. Sparkストリーミング処理<br/>3-2. Databricksオートローダーによる処理<br/>3-3. Delta live tableの連続パイプラインによる処理 |\n\nリアルタイム処理を実装する際には、システム間を直接連携（例： Kafka -> Spark ）させるのではなく、ストレージに対するオートローダーによる処理（例: Kafka -> ストレージ -> Databricks ）がコストや可用性の観点で有効な場合もある。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d2182755-4582-455b-a8c6-9e1aa89df400"}}},{"cell_type":"markdown","source":["## 3. メダリオンアーキテクチャによるデータエンジニアリング"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"08ce221f-c063-4ddc-a830-4259b4ebe2a1"}}},{"cell_type":"markdown","source":["メダリオンアーキテクチャとはソースシステムのデータを、Bronze、Silver、Goldの三層で管理する手法であり、それをベースにデータレイヤーの設計を行うべき。\n\nデータレイヤーに分けることにより、次のようなメリットがある。\n\n- データレイヤーごとの役割が明確となること\n- データ品質が担保されたデータの提供が可能となること\n- ローデータから再度テーブルを再作成できること\n\n参考リンク\n\n- [Medallion Architecture | Databricks](https://databricks.com/jp/glossary/medallion-architecture)\n- [What's Data Lake ? Azure Data Lake best practice - Speaker Deck](https://speakerdeck.com/ryomaru0825/whats-data-lake-azure-data-lake-best-practice?slide=18)\n\n\n次がレイヤーのサンプルである。\n\n| #    | データレイヤー | 概要                                                   | 類義語             |\n| ---- | -------------- | ------------------------------------------------------ | ------------------ |\n| 1    | Bronze         | 生データを保持するレイヤー                             | Raw、Data lake     |\n| 2    | Silver         | ソースシステムと同様の粒度で検証済みのデータを保持するレイヤー | Enriched、DWH      |\n| 3    | Gold           | 集計したデータを保持するレイヤー   | Curated、Data Mart |\n\n\n**Bronze の特徴について**\n\n- 取り込んだ生データのコピーを、履歴として保持。\n- データを削除する場合には、物理削除ではなく、論理削除が推奨。\n- スキーマ展開を許可するなどソースシステム側の変更対応を容易化。\n- データ型を文字型として保持するなどシステムエラーの発生を低減。\n\n**Silver の特徴について**\n- Bronze のデータに基づき、ソースシステムと同等のデータ粒度で保持。\n- スキーマを適用し、dropDuplicates 関数を利用した重複排除等によるデータ品質チェック処理を実施。\n\n\n**Gold の特徴について**\n- データ利活用（消費）の目的に合致するように編成・集計したデータを保持。\n- ACLや行レベルセキュリティ等のデータアクセス制御を考慮することが多い。\n\n参考リンク\n\n- [データ ランディング ゾーンごとに 3 つの Azure Data Lake Storage Gen2 アカウントをプロビジョニングする - Cloud Adoption Framework | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/cloud-adoption-framework/scenarios/data-management/best-practices/data-lake-services)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5bafe4ac-9f2f-4a8f-8ddc-947acf87b565"}}},{"cell_type":"markdown","source":["### 3-1. Bronze テーブルへのデータ書き込み例"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e8dac2a5-692b-4d37-b4e7-b74c3fd2b378"}}},{"cell_type":"code","source":["# 事前準備\ndb_name = 'sample_tpch'\nbrz_tbl_name = 'part_raw'\nbrz_tbl_full_name = f'{db_name}.{brz_tbl_name}'\n\nspark.sql(f'''\nCREATE DATABASE IF NOT EXISTS {db_name}\n''')\n\n# \nspark.sql(f\"\"\"\nCREATE OR REPLACE TABLE {brz_tbl_full_name}\n(\n  p_partkey string,\n  p_name string,\n  p_mfgr string,\n  p_brand string,\n  p_type string,\n  p_size string,\n  p_container string,\n  p_retailprice string,\n  p_comment string,\n  _datasource string,\n  _ingest_timestamp timestamp\n)\nUSING delta\n\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6b0e7b70-5853-4235-a6ac-9c4c12df7296"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[49]: DataFrame[]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[49]: DataFrame[]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["src_file_path = 'dbfs:/databricks-datasets/tpch/data-001/part/part.tbl'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1df72b4f-831c-4653-a814-7740f9382ab3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# ファイル内容の一部を確認\nprint(dbutils.fs.head(src_file_path, 1030))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17dd070e-e0b6-411d-8223-d425b76d5451"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[Truncated to first 1030 bytes]\n1|goldenrod lavender spring chocolate lace|Manufacturer#1|Brand#13|PROMO BURNISHED COPPER|7|JUMBO PKG|901.00|ly. slyly ironi|\n2|blush thistle blue yellow saddle|Manufacturer#1|Brand#13|LARGE BRUSHED BRASS|1|LG CASE|902.00|lar accounts amo|\n3|spring green yellow purple cornsilk|Manufacturer#4|Brand#42|STANDARD POLISHED BRASS|21|WRAP CASE|903.00|egular deposits hag|\n4|cornflower chocolate smoke green pink|Manufacturer#3|Brand#34|SMALL PLATED BRASS|14|MED DRUM|904.00|p furiously r|\n5|forest brown coral puff cream|Manufacturer#3|Brand#32|STANDARD POLISHED TIN|15|SM PKG|905.00| wake carefully |\n6|bisque cornflower lawn forest magenta|Manufacturer#2|Brand#24|PROMO PLATED STEEL|4|MED BAG|906.00|sual a|\n7|moccasin green thistle khaki floral|Manufacturer#1|Brand#11|SMALL PLATED COPPER|45|SM BAG|907.00|lyly. ex|\n8|misty lace thistle snow royal|Manufacturer#4|Brand#44|PROMO BURNISHED TIN|41|LG DRUM|908.00|eposi|\n9|thistle dim navajo dark gainsboro|Manufacturer#4|Brand#43|SMALL BURNISHED STEEL|12|WRAP CASE|909.00|ironic foxe|\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[Truncated to first 1030 bytes]\n1|goldenrod lavender spring chocolate lace|Manufacturer#1|Brand#13|PROMO BURNISHED COPPER|7|JUMBO PKG|901.00|ly. slyly ironi|\n2|blush thistle blue yellow saddle|Manufacturer#1|Brand#13|LARGE BRUSHED BRASS|1|LG CASE|902.00|lar accounts amo|\n3|spring green yellow purple cornsilk|Manufacturer#4|Brand#42|STANDARD POLISHED BRASS|21|WRAP CASE|903.00|egular deposits hag|\n4|cornflower chocolate smoke green pink|Manufacturer#3|Brand#34|SMALL PLATED BRASS|14|MED DRUM|904.00|p furiously r|\n5|forest brown coral puff cream|Manufacturer#3|Brand#32|STANDARD POLISHED TIN|15|SM PKG|905.00| wake carefully |\n6|bisque cornflower lawn forest magenta|Manufacturer#2|Brand#24|PROMO PLATED STEEL|4|MED BAG|906.00|sual a|\n7|moccasin green thistle khaki floral|Manufacturer#1|Brand#11|SMALL PLATED COPPER|45|SM BAG|907.00|lyly. ex|\n8|misty lace thistle snow royal|Manufacturer#4|Brand#44|PROMO BURNISHED TIN|41|LG DRUM|908.00|eposi|\n9|thistle dim navajo dark gainsboro|Manufacturer#4|Brand#43|SMALL BURNISHED STEEL|12|WRAP CASE|909.00|ironic foxe|\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from  pyspark.sql.functions import input_file_name,current_timestamp\n\n# ソースファイルから読み込み\ndf = (spark\n      .read\n      .format(\"csv\")\n      .option(\"header\", \"False\")\n      .option(\"inferSchema\", \"False\")\n      .option(\"sep\", \"|\")\n      .load(src_file_path)\n    )\n\n# ソースファイルにヘッダーがないため、カラム名を変更\nrenamed_cols_names = {\n    '_c0':'p_partkey',\n    '_c1':'p_name',\n    '_c2':'p_mfgr',\n    '_c3':'p_brand',\n    '_c4':'p_type',\n    '_c5':'p_size',\n    '_c6':'p_container',\n    '_c7':'p_retailprice',\n    '_c8':'p_comment',\n}\nfor existing_col,new_col in renamed_cols_names.items():\n    df = df.withColumnRenamed(existing_col, new_col)\n\n# 最後のカラムを削除\ndf = df.drop('_c9')\n\n# 監査列として、`_datasource`列と`_ingest_timestamp`列を追加\ndf = (df\n        .withColumn(\"_datasource\", input_file_name())\n        .withColumn(\"_ingest_timestamp\", current_timestamp())\n     )\n\n# `append`によりデータの書き込み\n(df.write\n     .format('delta')\n     .mode('append')\n     .option(\"mergeSchema\", \"true\")\n     .saveAsTable(brz_tbl_full_name)\n)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c8d2bf82-7f78-48e8-b9b0-03181f4876fa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# データを確認\nspark.table(brz_tbl_full_name).printSchema()\ndisplay(spark.table(brz_tbl_full_name).limit(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7243972d-188a-4fc2-acab-30b4d3d26088"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- p_partkey: string (nullable = true)\n |-- p_name: string (nullable = true)\n |-- p_mfgr: string (nullable = true)\n |-- p_brand: string (nullable = true)\n |-- p_type: string (nullable = true)\n |-- p_size: string (nullable = true)\n |-- p_container: string (nullable = true)\n |-- p_retailprice: string (nullable = true)\n |-- p_comment: string (nullable = true)\n |-- _datasource: string (nullable = true)\n |-- _ingest_timestamp: timestamp (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- p_partkey: string (nullable = true)\n-- p_name: string (nullable = true)\n-- p_mfgr: string (nullable = true)\n-- p_brand: string (nullable = true)\n-- p_type: string (nullable = true)\n-- p_size: string (nullable = true)\n-- p_container: string (nullable = true)\n-- p_retailprice: string (nullable = true)\n-- p_comment: string (nullable = true)\n-- _datasource: string (nullable = true)\n-- _ingest_timestamp: timestamp (nullable = true)\n\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["1","goldenrod lavender spring chocolate lace","Manufacturer#1","Brand#13","PROMO BURNISHED COPPER","7","JUMBO PKG","901.00","ly. slyly ironi","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:45:02.928+0000"],["2","blush thistle blue yellow saddle","Manufacturer#1","Brand#13","LARGE BRUSHED BRASS","1","LG CASE","902.00","lar accounts amo","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:45:02.928+0000"],["3","spring green yellow purple cornsilk","Manufacturer#4","Brand#42","STANDARD POLISHED BRASS","21","WRAP CASE","903.00","egular deposits hag","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:45:02.928+0000"],["4","cornflower chocolate smoke green pink","Manufacturer#3","Brand#34","SMALL PLATED BRASS","14","MED DRUM","904.00","p furiously r","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:45:02.928+0000"],["5","forest brown coral puff cream","Manufacturer#3","Brand#32","STANDARD POLISHED TIN","15","SM PKG","905.00"," wake carefully ","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:45:02.928+0000"],["6","bisque cornflower lawn forest magenta","Manufacturer#2","Brand#24","PROMO PLATED STEEL","4","MED BAG","906.00","sual a","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:45:02.928+0000"],["7","moccasin green thistle khaki floral","Manufacturer#1","Brand#11","SMALL PLATED COPPER","45","SM BAG","907.00","lyly. ex","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:45:02.928+0000"],["8","misty lace thistle snow royal","Manufacturer#4","Brand#44","PROMO BURNISHED TIN","41","LG DRUM","908.00","eposi","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:45:02.928+0000"],["9","thistle dim navajo dark gainsboro","Manufacturer#4","Brand#43","SMALL BURNISHED STEEL","12","WRAP CASE","909.00","ironic foxe","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:45:02.928+0000"],["10","linen pink saddle puff powder","Manufacturer#5","Brand#54","LARGE BURNISHED STEEL","44","LG CAN","910.01","ithely final deposit","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:45:02.928+0000"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"p_partkey","type":"\"string\"","metadata":"{}"},{"name":"p_name","type":"\"string\"","metadata":"{}"},{"name":"p_mfgr","type":"\"string\"","metadata":"{}"},{"name":"p_brand","type":"\"string\"","metadata":"{}"},{"name":"p_type","type":"\"string\"","metadata":"{}"},{"name":"p_size","type":"\"string\"","metadata":"{}"},{"name":"p_container","type":"\"string\"","metadata":"{}"},{"name":"p_retailprice","type":"\"string\"","metadata":"{}"},{"name":"p_comment","type":"\"string\"","metadata":"{}"},{"name":"_datasource","type":"\"string\"","metadata":"{}"},{"name":"_ingest_timestamp","type":"\"timestamp\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>p_partkey</th><th>p_name</th><th>p_mfgr</th><th>p_brand</th><th>p_type</th><th>p_size</th><th>p_container</th><th>p_retailprice</th><th>p_comment</th><th>_datasource</th><th>_ingest_timestamp</th></tr></thead><tbody><tr><td>1</td><td>goldenrod lavender spring chocolate lace</td><td>Manufacturer#1</td><td>Brand#13</td><td>PROMO BURNISHED COPPER</td><td>7</td><td>JUMBO PKG</td><td>901.00</td><td>ly. slyly ironi</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:45:02.928+0000</td></tr><tr><td>2</td><td>blush thistle blue yellow saddle</td><td>Manufacturer#1</td><td>Brand#13</td><td>LARGE BRUSHED BRASS</td><td>1</td><td>LG CASE</td><td>902.00</td><td>lar accounts amo</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:45:02.928+0000</td></tr><tr><td>3</td><td>spring green yellow purple cornsilk</td><td>Manufacturer#4</td><td>Brand#42</td><td>STANDARD POLISHED BRASS</td><td>21</td><td>WRAP CASE</td><td>903.00</td><td>egular deposits hag</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:45:02.928+0000</td></tr><tr><td>4</td><td>cornflower chocolate smoke green pink</td><td>Manufacturer#3</td><td>Brand#34</td><td>SMALL PLATED BRASS</td><td>14</td><td>MED DRUM</td><td>904.00</td><td>p furiously r</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:45:02.928+0000</td></tr><tr><td>5</td><td>forest brown coral puff cream</td><td>Manufacturer#3</td><td>Brand#32</td><td>STANDARD POLISHED TIN</td><td>15</td><td>SM PKG</td><td>905.00</td><td> wake carefully </td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:45:02.928+0000</td></tr><tr><td>6</td><td>bisque cornflower lawn forest magenta</td><td>Manufacturer#2</td><td>Brand#24</td><td>PROMO PLATED STEEL</td><td>4</td><td>MED BAG</td><td>906.00</td><td>sual a</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:45:02.928+0000</td></tr><tr><td>7</td><td>moccasin green thistle khaki floral</td><td>Manufacturer#1</td><td>Brand#11</td><td>SMALL PLATED COPPER</td><td>45</td><td>SM BAG</td><td>907.00</td><td>lyly. ex</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:45:02.928+0000</td></tr><tr><td>8</td><td>misty lace thistle snow royal</td><td>Manufacturer#4</td><td>Brand#44</td><td>PROMO BURNISHED TIN</td><td>41</td><td>LG DRUM</td><td>908.00</td><td>eposi</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:45:02.928+0000</td></tr><tr><td>9</td><td>thistle dim navajo dark gainsboro</td><td>Manufacturer#4</td><td>Brand#43</td><td>SMALL BURNISHED STEEL</td><td>12</td><td>WRAP CASE</td><td>909.00</td><td>ironic foxe</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:45:02.928+0000</td></tr><tr><td>10</td><td>linen pink saddle puff powder</td><td>Manufacturer#5</td><td>Brand#54</td><td>LARGE BURNISHED STEEL</td><td>44</td><td>LG CAN</td><td>910.01</td><td>ithely final deposit</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:45:02.928+0000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Spark SQL にて同等のことを実施することも可能"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2dd6c1f4-72c1-41a2-9b14-acbafd8e62af"}}},{"cell_type":"code","source":["%sql\n-- テキスト区切りファイルからデータを読み込む\nCREATE OR REPLACE TEMPORARY VIEW tmp_part_001\nUSING csv\nOPTIONS (\n  path 'dbfs:/databricks-datasets/tpch/data-001/part/part.tbl',\n  header false,\n  SEP '|'\n)\n;\n\n-- ソースファイルにヘッダーがないため、カラム名を変更\n-- 最後のカラム('_c9)を削除\nCREATE OR REPLACE TEMPORARY VIEW tmp_part_002\nAS\nSELECT\n  _c0 AS p_partkey,\n  _c1 AS p_name,\n  _c2 AS p_mfgr,\n  _c3 AS p_brand,\n  _c4 AS p_type,\n  _c5 AS p_size,\n  _c6 AS p_container,\n  _c7 AS p_retailprice,\n  _c8 AS p_comment\n  FROM\n    tmp_part_001\n;\n\n-- 監査列として、`_datasource`列と`_ingest_timestamp`列を追加\nCREATE OR REPLACE TEMPORARY VIEW tmp_part_003\nAS\nSELECT\n  *\n  ,input_file_name() AS _datasource\n  ,current_timestamp() AS _ingest_timestamp\n  FROM \n    tmp_part_002\n;"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"37590a03-6aa5-400e-9d88-af8e7084e141"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.conf.set('da.brz_tbl_full_name', brz_tbl_full_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a5178b15-db63-4311-bdcc-5bdf6416aaa2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\n-- `append`によりデータの書き込み\n\n-- スキーマ展開を許可\nSET spark.databricks.delta.schema.autoMerge = True;\n\nINSERT INTO  ${da.brz_tbl_full_name}\nSELECT\n  *\n  FROM\n    tmp_part_003"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f458363f-a1aa-43c1-ab9f-940bc6fac75e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[1000000,1000000]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"num_affected_rows","type":"\"long\"","metadata":"{}"},{"name":"num_inserted_rows","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>1000000</td><td>1000000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT\n  version,\n  operation,\n  operationParameters\n\n  FROM (\n    DESCRIBE HISTORY sample_tpch.part_raw\n  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ec1f535-4b13-4a25-a6f0-793e5e42b983"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[2,"WRITE",{"mode":"Append","partitionBy":"[]"}],[1,"WRITE",{"mode":"Append","partitionBy":"[]"}],[0,"CREATE OR REPLACE TABLE",{"isManaged":"true","description":null,"partitionBy":"[]","properties":"{}"}]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"version","type":"\"long\"","metadata":"{}"},{"name":"operation","type":"\"string\"","metadata":"{}"},{"name":"operationParameters","type":"{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>operation</th><th>operationParameters</th></tr></thead><tbody><tr><td>2</td><td>WRITE</td><td>Map(mode -> Append, partitionBy -> [])</td></tr><tr><td>1</td><td>WRITE</td><td>Map(mode -> Append, partitionBy -> [])</td></tr><tr><td>0</td><td>CREATE OR REPLACE TABLE</td><td>Map(isManaged -> true, description -> null, partitionBy -> [], properties -> {})</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 3-2. Silver テーブルへのデータ書き込み例"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c098b2f7-b3db-4a0f-a600-83d1c0749696"}}},{"cell_type":"code","source":["# 事前準備\nslv_tbl_name = 'part'\nslv_tbl_full_name = f'{db_name}.{slv_tbl_name}'\n\nspark.sql(f\"\"\"\nCREATE OR REPLACE TABLE {slv_tbl_full_name}\n(\n  p_partkey long,\n  p_name string,\n  p_mfgr string,\n  p_brand string,\n  p_type string,\n  p_size int,\n  p_container string,\n  p_retailprice decimal(12, 2),\n  p_comment string,\n  _datasource STRING,\n  _ingest_timestamp timestamp\n)\nUSING delta\n\"\"\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49cca03a-1886-4e5d-a1b4-2c1c9a1ff82b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[58]: DataFrame[]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[58]: DataFrame[]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from  pyspark.sql.functions import current_timestamp,lit\n\n# 下記の処理を実行したデータフレーム（df）を作成\n## 1. ブロンズテーブルから主キー（`p_partkey`）ごとに`_ingest_timestamp`列の最大日を抽出したサブセットを作成\n## 2. 主キー＋`_ingest_timestamp`列の条件で、1のサブセットとブロンズテーブルを結合\n## 3. ブロンズテーブルのデータ型をシルバーテーブルと同一のデータ型に変換\nbrz_to_slv_sql = f'''\nwith slv_records (\n  SELECT\n    p_partkey,\n    MAX(_ingest_timestamp) AS max_ingest_timestamp\n    \n    FROM\n      {brz_tbl_full_name}\n    GROUP BY\n      p_partkey      \n)\n\nSELECT\n  brz.p_partkey::long,\n  brz.p_name,\n  brz.p_mfgr,\n  brz.p_brand,\n  brz.p_type,\n  brz.p_size::int,\n  brz.p_container,\n  brz.p_retailprice::decimal(12, 2),\n  brz.p_comment,\n  brz._datasource,\n  brz._ingest_timestamp\n  \n  FROM\n    {brz_tbl_full_name} AS brz\n  INNER JOIN \n    slv_records AS slv\n    ON \n      brz.p_partkey =  slv.p_partkey\n      AND brz._ingest_timestamp =  slv.max_ingest_timestamp\n'''\ndf = spark.sql(brz_to_slv_sql)\n\n# dropDuplicates関数にて、主キーの一意性を保証。連携日ごとの一意性が保証されないことがあるため。\ndf = df.drop_duplicates(['p_partkey'])\n\n\n\n# 一時ビューからシルバーテーブルに対して、MERGE文によりアップサート処理を実施。\n# 一時ビューの`_ingest_timestamp`列がシルバーテーブルの`_ingest_timestamp`列以降である場合のみ、UPDATE処理を実行。\n\n# 一時ビューを作成\ntemp_view_name = f'_tmp_{brz_tbl_name}'\ndf.createOrReplaceTempView(temp_view_name)\n\n\n# Merge処理を実行\nspark.sql(f'''\nMERGE INTO {slv_tbl_full_name} AS tgt\n  USING {temp_view_name} AS src\n  \n  ON tgt.p_partkey = src.p_partkey\n  \n  WHEN MATCHED\n  AND tgt._ingest_timestamp < src._ingest_timestamp\n    THEN UPDATE SET *\n  WHEN NOT MATCHED\n    THEN INSERT *\n''')\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba1e4123-620f-4463-a744-9902d0e3f39b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[59]: DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[59]: DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.table(slv_tbl_full_name).limit(10).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e366f4c-ac83-41fa-b232-d89aae4eb94e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[6,"bisque cornflower lawn forest magenta","Manufacturer#2","Brand#24","PROMO PLATED STEEL",4,"MED BAG","906.00","sual a","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:45:15.619+0000"],[7,"moccasin green thistle khaki floral","Manufacturer#1","Brand#11","SMALL PLATED COPPER",45,"SM BAG","907.00","lyly. ex","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:45:15.619+0000"],[19,"chocolate navy tan deep brown","Manufacturer#2","Brand#23","SMALL ANODIZED NICKEL",33,"WRAP BOX","919.01"," pending acc","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:45:15.619+0000"],[22,"medium forest blue ghost black","Manufacturer#4","Brand#43","PROMO POLISHED BRASS",19,"LG DRUM","922.02"," even p","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:45:15.619+0000"],[25,"aquamarine steel firebrick light turquoise","Manufacturer#5","Brand#55","STANDARD BRUSHED COPPER",3,"JUMBO BAG","925.02","requests wake","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:45:15.619+0000"],[26,"beige frosted moccasin chocolate snow","Manufacturer#3","Brand#32","SMALL BRUSHED STEEL",32,"SM CASE","926.02"," instructions i","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:45:15.619+0000"],[29,"lemon sky grey salmon orchid","Manufacturer#3","Brand#33","PROMO PLATED COPPER",7,"LG DRUM","929.02"," carefully fluffi","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:45:15.619+0000"],[31,"slate seashell steel medium moccasin","Manufacturer#5","Brand#53","STANDARD BRUSHED TIN",10,"LG BAG","931.03","uriously s","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:45:15.619+0000"],[32,"sandy wheat coral spring burnished","Manufacturer#4","Brand#42","ECONOMY PLATED BRASS",31,"LG CASE","932.03","urts. carefully fin","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:45:15.619+0000"],[34,"khaki steel rose ghost salmon","Manufacturer#1","Brand#13","LARGE BRUSHED STEEL",8,"JUMBO BOX","934.03","riously ironic","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:45:15.619+0000"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"p_partkey","type":"\"long\"","metadata":"{}"},{"name":"p_name","type":"\"string\"","metadata":"{}"},{"name":"p_mfgr","type":"\"string\"","metadata":"{}"},{"name":"p_brand","type":"\"string\"","metadata":"{}"},{"name":"p_type","type":"\"string\"","metadata":"{}"},{"name":"p_size","type":"\"integer\"","metadata":"{}"},{"name":"p_container","type":"\"string\"","metadata":"{}"},{"name":"p_retailprice","type":"\"decimal(12,2)\"","metadata":"{}"},{"name":"p_comment","type":"\"string\"","metadata":"{}"},{"name":"_datasource","type":"\"string\"","metadata":"{}"},{"name":"_ingest_timestamp","type":"\"timestamp\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>p_partkey</th><th>p_name</th><th>p_mfgr</th><th>p_brand</th><th>p_type</th><th>p_size</th><th>p_container</th><th>p_retailprice</th><th>p_comment</th><th>_datasource</th><th>_ingest_timestamp</th></tr></thead><tbody><tr><td>6</td><td>bisque cornflower lawn forest magenta</td><td>Manufacturer#2</td><td>Brand#24</td><td>PROMO PLATED STEEL</td><td>4</td><td>MED BAG</td><td>906.00</td><td>sual a</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:45:15.619+0000</td></tr><tr><td>7</td><td>moccasin green thistle khaki floral</td><td>Manufacturer#1</td><td>Brand#11</td><td>SMALL PLATED COPPER</td><td>45</td><td>SM BAG</td><td>907.00</td><td>lyly. ex</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:45:15.619+0000</td></tr><tr><td>19</td><td>chocolate navy tan deep brown</td><td>Manufacturer#2</td><td>Brand#23</td><td>SMALL ANODIZED NICKEL</td><td>33</td><td>WRAP BOX</td><td>919.01</td><td> pending acc</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:45:15.619+0000</td></tr><tr><td>22</td><td>medium forest blue ghost black</td><td>Manufacturer#4</td><td>Brand#43</td><td>PROMO POLISHED BRASS</td><td>19</td><td>LG DRUM</td><td>922.02</td><td> even p</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:45:15.619+0000</td></tr><tr><td>25</td><td>aquamarine steel firebrick light turquoise</td><td>Manufacturer#5</td><td>Brand#55</td><td>STANDARD BRUSHED COPPER</td><td>3</td><td>JUMBO BAG</td><td>925.02</td><td>requests wake</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:45:15.619+0000</td></tr><tr><td>26</td><td>beige frosted moccasin chocolate snow</td><td>Manufacturer#3</td><td>Brand#32</td><td>SMALL BRUSHED STEEL</td><td>32</td><td>SM CASE</td><td>926.02</td><td> instructions i</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:45:15.619+0000</td></tr><tr><td>29</td><td>lemon sky grey salmon orchid</td><td>Manufacturer#3</td><td>Brand#33</td><td>PROMO PLATED COPPER</td><td>7</td><td>LG DRUM</td><td>929.02</td><td> carefully fluffi</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:45:15.619+0000</td></tr><tr><td>31</td><td>slate seashell steel medium moccasin</td><td>Manufacturer#5</td><td>Brand#53</td><td>STANDARD BRUSHED TIN</td><td>10</td><td>LG BAG</td><td>931.03</td><td>uriously s</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:45:15.619+0000</td></tr><tr><td>32</td><td>sandy wheat coral spring burnished</td><td>Manufacturer#4</td><td>Brand#42</td><td>ECONOMY PLATED BRASS</td><td>31</td><td>LG CASE</td><td>932.03</td><td>urts. carefully fin</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:45:15.619+0000</td></tr><tr><td>34</td><td>khaki steel rose ghost salmon</td><td>Manufacturer#1</td><td>Brand#13</td><td>LARGE BRUSHED STEEL</td><td>8</td><td>JUMBO BOX</td><td>934.03</td><td>riously ironic</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:45:15.619+0000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 3-3. Gold テーブルへのデータ書き込み例"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f61409e4-94c8-408b-b5ce-61e97d14c293"}}},{"cell_type":"code","source":["# 事前準備\ngld_tbl_name = 'part_counts_by_mfgr'\ngld_tbl_full_name = f'{db_name}.{gld_tbl_name}'\n\n# `p_mfgr`ごとのカウント数を保持したデータフレームを定義\nslv_to_gld_sql = f\"\"\"\nSELECT\n  p_mfgr,\n  count(*) AS part_counts\n  \n  FROM\n    {slv_tbl_full_name}\n  GROUP BY\n    p_mfgr    \n\"\"\"\ndf = spark.sql(slv_to_gld_sql)\n\n\n# CTAS（CREAT TABLE AS SLECT）により、テーブルを作成。\n## 一時ビューを作成\ntmp_view_name = f'_tmp_{slv_tbl_name}'\ndf.createOrReplaceTempView(tmp_view_name)\n\n## CTASを実行\nctas_sql = f'''\ncreate or replace table {gld_tbl_full_name}\n  using delta\n  TBLPROPERTIES (\n    delta.autoOptimize.optimizeWrite = True, \n    delta.autoOptimize.autoCompact   = True,\n    delta.dataSkippingNumIndexedCols = 1\n  )\n  AS \n  select \n    * \n    from \n      {tmp_view_name}\n'''\nspark.sql(ctas_sql)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92a95251-8ffd-4221-9494-4cc7756fb430"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[61]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[61]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark.table(gld_tbl_full_name).limit(10).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3527e511-9966-4b78-8d7d-8c6ab8a685a6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["Manufacturer#1",199931],["Manufacturer#5",199591],["Manufacturer#4",200305],["Manufacturer#3",200567],["Manufacturer#2",199606]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"p_mfgr","type":"\"string\"","metadata":"{}"},{"name":"part_counts","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>p_mfgr</th><th>part_counts</th></tr></thead><tbody><tr><td>Manufacturer#1</td><td>199931</td></tr><tr><td>Manufacturer#5</td><td>199591</td></tr><tr><td>Manufacturer#4</td><td>200305</td></tr><tr><td>Manufacturer#3</td><td>200567</td></tr><tr><td>Manufacturer#2</td><td>199606</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 4. Spark によるデータエンジニアリングに利用すべきプログラミング言語とライブラリー"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"350c1d86-d500-471a-a993-0ab28a2cfa35"}}},{"cell_type":"markdown","source":["### 4-1. PySpark、および、Spark SQLの利用が推奨"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b0e34e7b-6934-4f28-b6c8-306b2510137c"}}},{"cell_type":"markdown","source":["Spark では、ScalaやRなどの多様な言語で実装できるが、プログラミング言語の普及率やSparkに関する情報量の観点で、PySpark、および、Spark SQL を用いることが推奨。<br>\n異なる言語でデータフレームのやり取りを行う際には、 Spark 一時ビュー経由で行う方法が容易である。\n\nデータエンジニアリング時には、PySpark、あるいは、Spark SQL のいずれかでのみでの実施も可能であるが、変数やコンポーネント化を実施するために Python の基本的なスキルが必要となる。<br>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d150d0b7-1236-4305-9d8e-208165115691"}}},{"cell_type":"markdown","source":["### 4-2. PySpark(Spark SQL)、pandas-on-Spark、Pandas の利用指針"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b14111b7-5123-4d1e-b02d-997c61ca027a"}}},{"cell_type":"markdown","source":["PySpark(Spark SQL)、pandas-on-Spark、Pandas の順での利用が推奨。PySpark -> pandas-on-Spark -> PySpark の順で利用する場合には性能に課題をかかえることがある。\n\nExcel や SAS などのファイルを読み込む際には、Pandas で読み込み、PySpark に変換することで追加のコンポーネントが必要なくなる場合があり、データ量を想定した上で対応方針を検討する。\n- [Input/output — pandas 1.4.3 documentation (pydata.org)](https://pandas.pydata.org/docs/reference/io.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e09772e-f1a4-439a-8aa8-440d987ed138"}}},{"cell_type":"markdown","source":["## 5. 処理の共通化"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b803aa5-23db-4f4c-8677-f38b37b52061"}}},{"cell_type":"markdown","source":["### 5-1. 処理を共通化する方法"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c349e3b9-9a6d-4506-89df-0da2b0c1dbd8"}}},{"cell_type":"markdown","source":["システム規模やメンバーのスキルセットに応じて、次のような開発方針を定める。<br>\n\n1. PySpark、あるいは、Spark SQL により個別処理を実装する方法\n2. Python により処理を共通化して実装する方法\n3. 共通化処理を保持させたライブラリーを用いて実装する方法\n\n`共通化処理を保持させたライブラリーを用いた実装`を複数チームで行う場合には、複数プロジェクトの総意をリードするCoE（center of excellence）のような組織が必要となるなど難易度は高い。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c1d6a84-b4ff-4e17-8247-38a3b0eb63cc"}}},{"cell_type":"markdown","source":["#### 1. PySpark、あるいは、Spark SQL により個別処理を実装する方法"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"93643acf-d70a-4237-b553-5c0dfe9ee03b"}}},{"cell_type":"code","source":["from  pyspark.sql.functions import input_file_name,current_timestamp\n\n# ソースファイルから読み込み\ndf = (spark\n      .read\n      .format(\"csv\")\n      .option(\"header\", \"False\")\n      .option(\"inferSchema\", \"False\")\n      .option(\"sep\", \"|\")\n      .load(src_file_path)\n    )\n\n# ソースファイルにヘッダーがないため、カラム名を変更\nrenamed_cols_names = {\n    '_c0':'p_partkey',\n    '_c1':'p_name',\n    '_c2':'p_mfgr',\n    '_c3':'p_brand',\n    '_c4':'p_type',\n    '_c5':'p_size',\n    '_c6':'p_container',\n    '_c7':'p_retailprice',\n    '_c8':'p_comment',\n}\nfor existing_col,new_col in renamed_cols_names.items():\n    df = df.withColumnRenamed(existing_col, new_col)\n\n# 最後のカラムを削除\ndropped_cols_names = ['_c9']\nfor dropped_cols_name in dropped_cols_names:\n    df = df.drop(dropped_cols_name)\n\n# 監査列として、`_datasource`列と`_ingest_timestamp`列を追加\ndf = (df\n      .withColumn(\"_datasource\", input_file_name())\n       .withColumn(\"_ingest_timestamp\", current_timestamp())\n     )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"856eb45e-db97-49dd-8b68-dac346d49100"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### 2. Python により処理を共通化して実装する方法"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"038e2ebf-9e78-4eed-a8ae-227e693076ae"}}},{"cell_type":"code","source":["# 共通化した関数\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import input_file_name,current_timestamp\n\ndef read_from_csv(\n    header,\n    inferSchema,\n    sep,\n    src_file_path,\n):\n    spark = SparkSession.getActiveSession()\n    return (\n        spark\n        .read\n        .format(\"csv\")\n        .option(\"header\", header)\n        .option(\"inferSchema\", inferSchema)\n        .option(\"sep\", sep)\n        .load(src_file_path)\n    )\n\ndef rename_cols(\n    df,\n    renamed_cols_names,\n):\n    for existing_col,new_col in renamed_cols_names.items():\n        df = df.withColumnRenamed(existing_col, new_col)\n    return df\n\ndef drop_cols(\n    df,\n    cols,\n):\n    for col in cols:\n        df = df.drop(col)\n    return df\n\ndef with_audit_cols(\n    df,\n):\n    return (\n        df\n        .withColumn(\"_datasource\", input_file_name())\n        .withColumn(\"_ingest_timestamp\", current_timestamp())\n    )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e1c3a6d-8e6d-403b-869c-61a098f23d8c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 個別処理の設定値\nsrc_file_path = 'dbfs:/databricks-datasets/tpch/data-001/part/part.tbl'"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74639e00-a1f6-4ed8-97ac-7e17eb003b5b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# データエンジニアリングのコード\nspark = SparkSession.builder.getOrCreate()\n\ndf_1 = read_from_csv(\n    header = \"False\",\n    inferSchema = \"False\",\n    sep = \"|\",\n    src_file_path = src_file_path,\n)\n\nrenamed_cols_names = {\n    '_c0':'p_partkey',\n    '_c1':'p_name',\n    '_c2':'p_mfgr',\n    '_c3':'p_brand',\n    '_c4':'p_type',\n    '_c5':'p_size',\n    '_c6':'p_container',\n    '_c7':'p_retailprice',\n    '_c8':'p_comment',\n}\ndf_2 = rename_cols(df_1,renamed_cols_names) \n\ndropped_cols_names = ['_c9']\ndf_3 = drop_cols(df_2,dropped_cols_names)\n\ndf_4 = with_audit_cols(df_3)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46b098d4-9a33-4570-a981-fd2a0e436bcc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# データを確認\ndf_4.limit(10).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c8cabf1-5370-431f-8592-fb1a5f76c5fc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["1","goldenrod lavender spring chocolate lace","Manufacturer#1","Brand#13","PROMO BURNISHED COPPER","7","JUMBO PKG","901.00","ly. slyly ironi","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:46:01.456+0000"],["2","blush thistle blue yellow saddle","Manufacturer#1","Brand#13","LARGE BRUSHED BRASS","1","LG CASE","902.00","lar accounts amo","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:46:01.456+0000"],["3","spring green yellow purple cornsilk","Manufacturer#4","Brand#42","STANDARD POLISHED BRASS","21","WRAP CASE","903.00","egular deposits hag","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:46:01.456+0000"],["4","cornflower chocolate smoke green pink","Manufacturer#3","Brand#34","SMALL PLATED BRASS","14","MED DRUM","904.00","p furiously r","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:46:01.456+0000"],["5","forest brown coral puff cream","Manufacturer#3","Brand#32","STANDARD POLISHED TIN","15","SM PKG","905.00"," wake carefully ","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:46:01.456+0000"],["6","bisque cornflower lawn forest magenta","Manufacturer#2","Brand#24","PROMO PLATED STEEL","4","MED BAG","906.00","sual a","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:46:01.456+0000"],["7","moccasin green thistle khaki floral","Manufacturer#1","Brand#11","SMALL PLATED COPPER","45","SM BAG","907.00","lyly. ex","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:46:01.456+0000"],["8","misty lace thistle snow royal","Manufacturer#4","Brand#44","PROMO BURNISHED TIN","41","LG DRUM","908.00","eposi","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:46:01.456+0000"],["9","thistle dim navajo dark gainsboro","Manufacturer#4","Brand#43","SMALL BURNISHED STEEL","12","WRAP CASE","909.00","ironic foxe","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:46:01.456+0000"],["10","linen pink saddle puff powder","Manufacturer#5","Brand#54","LARGE BURNISHED STEEL","44","LG CAN","910.01","ithely final deposit","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:46:01.456+0000"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"p_partkey","type":"\"string\"","metadata":"{}"},{"name":"p_name","type":"\"string\"","metadata":"{}"},{"name":"p_mfgr","type":"\"string\"","metadata":"{}"},{"name":"p_brand","type":"\"string\"","metadata":"{}"},{"name":"p_type","type":"\"string\"","metadata":"{}"},{"name":"p_size","type":"\"string\"","metadata":"{}"},{"name":"p_container","type":"\"string\"","metadata":"{}"},{"name":"p_retailprice","type":"\"string\"","metadata":"{}"},{"name":"p_comment","type":"\"string\"","metadata":"{}"},{"name":"_datasource","type":"\"string\"","metadata":"{}"},{"name":"_ingest_timestamp","type":"\"timestamp\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>p_partkey</th><th>p_name</th><th>p_mfgr</th><th>p_brand</th><th>p_type</th><th>p_size</th><th>p_container</th><th>p_retailprice</th><th>p_comment</th><th>_datasource</th><th>_ingest_timestamp</th></tr></thead><tbody><tr><td>1</td><td>goldenrod lavender spring chocolate lace</td><td>Manufacturer#1</td><td>Brand#13</td><td>PROMO BURNISHED COPPER</td><td>7</td><td>JUMBO PKG</td><td>901.00</td><td>ly. slyly ironi</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:46:01.456+0000</td></tr><tr><td>2</td><td>blush thistle blue yellow saddle</td><td>Manufacturer#1</td><td>Brand#13</td><td>LARGE BRUSHED BRASS</td><td>1</td><td>LG CASE</td><td>902.00</td><td>lar accounts amo</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:46:01.456+0000</td></tr><tr><td>3</td><td>spring green yellow purple cornsilk</td><td>Manufacturer#4</td><td>Brand#42</td><td>STANDARD POLISHED BRASS</td><td>21</td><td>WRAP CASE</td><td>903.00</td><td>egular deposits hag</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:46:01.456+0000</td></tr><tr><td>4</td><td>cornflower chocolate smoke green pink</td><td>Manufacturer#3</td><td>Brand#34</td><td>SMALL PLATED BRASS</td><td>14</td><td>MED DRUM</td><td>904.00</td><td>p furiously r</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:46:01.456+0000</td></tr><tr><td>5</td><td>forest brown coral puff cream</td><td>Manufacturer#3</td><td>Brand#32</td><td>STANDARD POLISHED TIN</td><td>15</td><td>SM PKG</td><td>905.00</td><td> wake carefully </td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:46:01.456+0000</td></tr><tr><td>6</td><td>bisque cornflower lawn forest magenta</td><td>Manufacturer#2</td><td>Brand#24</td><td>PROMO PLATED STEEL</td><td>4</td><td>MED BAG</td><td>906.00</td><td>sual a</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:46:01.456+0000</td></tr><tr><td>7</td><td>moccasin green thistle khaki floral</td><td>Manufacturer#1</td><td>Brand#11</td><td>SMALL PLATED COPPER</td><td>45</td><td>SM BAG</td><td>907.00</td><td>lyly. ex</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:46:01.456+0000</td></tr><tr><td>8</td><td>misty lace thistle snow royal</td><td>Manufacturer#4</td><td>Brand#44</td><td>PROMO BURNISHED TIN</td><td>41</td><td>LG DRUM</td><td>908.00</td><td>eposi</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:46:01.456+0000</td></tr><tr><td>9</td><td>thistle dim navajo dark gainsboro</td><td>Manufacturer#4</td><td>Brand#43</td><td>SMALL BURNISHED STEEL</td><td>12</td><td>WRAP CASE</td><td>909.00</td><td>ironic foxe</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:46:01.456+0000</td></tr><tr><td>10</td><td>linen pink saddle puff powder</td><td>Manufacturer#5</td><td>Brand#54</td><td>LARGE BURNISHED STEEL</td><td>44</td><td>LG CAN</td><td>910.01</td><td>ithely final deposit</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:46:01.456+0000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### 3. 共通化処理を保持させたライブラリーを用いて実装する方法"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5148c683-705d-4938-800a-7d902b4458f7"}}},{"cell_type":"code","source":["# クラスを定義\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import input_file_name,current_timestamp\n\nclass DataEngineering:\n\n    def __init__(self):\n        # `data_engineer_main*`で利用している変数\n        self.spark = SparkSession.builder.getOrCreate()\n        self.read_stage_dataframes = {}\n\n    def data_engineering_main(\n            self,\n            data_eng_conf,\n        ):\n        read_stage_dataframes = {}\n        for tasks_config_name, tasks_config_value in data_eng_conf.items():\n            method_name = tasks_config_value['method']\n            task_options = tasks_config_value['task_options']\n\n            # メソッド実行時の引数の文字列を生成\n            task_options_paras_list = []\n            # ２回目以降のループ時のみ、以前の処理結果のデータフレームを取得\n            if self.read_stage_dataframes != {}:\n                read_stage_df = list(self.read_stage_dataframes.values())[-1]\n                task_options_paras_list.append('tgt_df=read_stage_df')\n\n            for task_options_key, task_options_value in task_options.items():\n                # 引数が文字の場合には、`\"\"\"`で囲む\n                if type(task_options_value) == str:\n                    task_options_value = f'\"\"\"{task_options_value}\"\"\"'\n                task_options_paras_list.append(f'{task_options_key}={task_options_value}')\n            task_options_paras = ','.join(task_options_paras_list)\n\n            read_stage_dataframe = eval(f'{method_name}({task_options_paras})')\n            self.read_stage_dataframes[tasks_config_name] = read_stage_dataframe\n\n    @staticmethod\n    def read_from_csv(\n        header,\n        inferSchema,\n        sep,\n        src_file_path,\n    ):\n        spark = SparkSession.getActiveSession()\n        return (\n            spark\n            .read\n            .format(\"csv\")\n            .option(\"header\", header)\n            .option(\"inferSchema\", inferSchema)\n            .option(\"sep\", sep)\n            .load(src_file_path)\n        )\n\n    @staticmethod\n    def rename_cols(\n        tgt_df,\n        renamed_cols_names,\n    ):\n        for existing_col,new_col in renamed_cols_names.items():\n            tgt_df = tgt_df.withColumnRenamed(existing_col, new_col)\n        return tgt_df\n\n    @staticmethod\n    def drop_cols(\n        tgt_df,\n        dropped_cols_names,\n    ):\n        for col in dropped_cols_names:\n            tgt_df = tgt_df.drop(col)\n        return tgt_df\n\n    @staticmethod\n    def with_audit_cols(\n        tgt_df,\n    ):\n        return (\n            tgt_df\n            .withColumn(\"_datasource\", input_file_name())\n            .withColumn(\"_ingest_timestamp\", current_timestamp())\n        )\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bdf362f9-0896-4332-b628-0af9e408fbf3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# config を設定\ndata_eng_conf = {\n    'read_from_data_source': {\n        'method': 'DataEngineering.read_from_csv',\n        'task_options': {\n            'header': 'False',\n            'inferSchema': 'False',\n            'sep': '|',\n            'src_file_path': 'dbfs:/databricks-datasets/tpch/data-001/part/part.tbl',\n        },\n    },\n    'rename_cols_names': {\n        'method': 'DataEngineering.rename_cols',\n        'task_options': {\n            'renamed_cols_names': {\n                '_c0':'p_partkey',\n                '_c1':'p_name',\n                '_c2':'p_mfgr',\n                '_c3':'p_brand',\n                '_c4':'p_type',\n                '_c5':'p_size',\n                '_c6':'p_container',\n                '_c7':'p_retailprice',\n                '_c8':'p_comment',\n            },\n        },\n    },\n    'drop_cols': {\n        'method': 'DataEngineering.drop_cols',\n        'task_options': {\n            'dropped_cols_names': ['_c9'],\n        },\n    },\n    'with_audit_cols': {\n        'method': 'DataEngineering.with_audit_cols',\n        'task_options': {},\n    },\n}"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7a9986f5-306f-41a2-824d-fb4e4cce9b69"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 処理を実行\ndata_engineering = DataEngineering()\ndata_engineering.data_engineering_main(data_eng_conf)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c0d86bcc-9efc-4a50-91e6-d6df655dcedd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 実行計画を確認\nimport pprint\npprint.pprint(data_engineering.read_stage_dataframes)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2509771c-ed14-48a2-ab65-c9f6ce30b197"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">{&#39;drop_cols&#39;: DataFrame[p_partkey: string, p_name: string, p_mfgr: string, p_brand: string, p_type: string, p_size: string, p_container: string, p_retailprice: string, p_comment: string],\n &#39;read_from_data_source&#39;: DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string],\n &#39;rename_cols_names&#39;: DataFrame[p_partkey: string, p_name: string, p_mfgr: string, p_brand: string, p_type: string, p_size: string, p_container: string, p_retailprice: string, p_comment: string, _c9: string],\n &#39;with_audit_cols&#39;: DataFrame[p_partkey: string, p_name: string, p_mfgr: string, p_brand: string, p_type: string, p_size: string, p_container: string, p_retailprice: string, p_comment: string, _datasource: string, _ingest_timestamp: timestamp]}\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{&#39;drop_cols&#39;: DataFrame[p_partkey: string, p_name: string, p_mfgr: string, p_brand: string, p_type: string, p_size: string, p_container: string, p_retailprice: string, p_comment: string],\n &#39;read_from_data_source&#39;: DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string],\n &#39;rename_cols_names&#39;: DataFrame[p_partkey: string, p_name: string, p_mfgr: string, p_brand: string, p_type: string, p_size: string, p_container: string, p_retailprice: string, p_comment: string, _c9: string],\n &#39;with_audit_cols&#39;: DataFrame[p_partkey: string, p_name: string, p_mfgr: string, p_brand: string, p_type: string, p_size: string, p_container: string, p_retailprice: string, p_comment: string, _datasource: string, _ingest_timestamp: timestamp]}\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 最終的なデータフレームを確認\ndisplay(list(data_engineering.read_stage_dataframes.values())[-1].limit(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90730bdd-0cb9-4f43-97b6-7dea49a1cf03"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["1","goldenrod lavender spring chocolate lace","Manufacturer#1","Brand#13","PROMO BURNISHED COPPER","7","JUMBO PKG","901.00","ly. slyly ironi","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:46:06.616+0000"],["2","blush thistle blue yellow saddle","Manufacturer#1","Brand#13","LARGE BRUSHED BRASS","1","LG CASE","902.00","lar accounts amo","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:46:06.616+0000"],["3","spring green yellow purple cornsilk","Manufacturer#4","Brand#42","STANDARD POLISHED BRASS","21","WRAP CASE","903.00","egular deposits hag","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:46:06.616+0000"],["4","cornflower chocolate smoke green pink","Manufacturer#3","Brand#34","SMALL PLATED BRASS","14","MED DRUM","904.00","p furiously r","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:46:06.616+0000"],["5","forest brown coral puff cream","Manufacturer#3","Brand#32","STANDARD POLISHED TIN","15","SM PKG","905.00"," wake carefully ","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:46:06.616+0000"],["6","bisque cornflower lawn forest magenta","Manufacturer#2","Brand#24","PROMO PLATED STEEL","4","MED BAG","906.00","sual a","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:46:06.616+0000"],["7","moccasin green thistle khaki floral","Manufacturer#1","Brand#11","SMALL PLATED COPPER","45","SM BAG","907.00","lyly. ex","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:46:06.616+0000"],["8","misty lace thistle snow royal","Manufacturer#4","Brand#44","PROMO BURNISHED TIN","41","LG DRUM","908.00","eposi","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:46:06.616+0000"],["9","thistle dim navajo dark gainsboro","Manufacturer#4","Brand#43","SMALL BURNISHED STEEL","12","WRAP CASE","909.00","ironic foxe","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:46:06.616+0000"],["10","linen pink saddle puff powder","Manufacturer#5","Brand#54","LARGE BURNISHED STEEL","44","LG CAN","910.01","ithely final deposit","dbfs:/databricks-datasets/tpch/data-001/part/part.tbl","2022-10-05T00:46:06.616+0000"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"p_partkey","type":"\"string\"","metadata":"{}"},{"name":"p_name","type":"\"string\"","metadata":"{}"},{"name":"p_mfgr","type":"\"string\"","metadata":"{}"},{"name":"p_brand","type":"\"string\"","metadata":"{}"},{"name":"p_type","type":"\"string\"","metadata":"{}"},{"name":"p_size","type":"\"string\"","metadata":"{}"},{"name":"p_container","type":"\"string\"","metadata":"{}"},{"name":"p_retailprice","type":"\"string\"","metadata":"{}"},{"name":"p_comment","type":"\"string\"","metadata":"{}"},{"name":"_datasource","type":"\"string\"","metadata":"{}"},{"name":"_ingest_timestamp","type":"\"timestamp\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>p_partkey</th><th>p_name</th><th>p_mfgr</th><th>p_brand</th><th>p_type</th><th>p_size</th><th>p_container</th><th>p_retailprice</th><th>p_comment</th><th>_datasource</th><th>_ingest_timestamp</th></tr></thead><tbody><tr><td>1</td><td>goldenrod lavender spring chocolate lace</td><td>Manufacturer#1</td><td>Brand#13</td><td>PROMO BURNISHED COPPER</td><td>7</td><td>JUMBO PKG</td><td>901.00</td><td>ly. slyly ironi</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:46:06.616+0000</td></tr><tr><td>2</td><td>blush thistle blue yellow saddle</td><td>Manufacturer#1</td><td>Brand#13</td><td>LARGE BRUSHED BRASS</td><td>1</td><td>LG CASE</td><td>902.00</td><td>lar accounts amo</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:46:06.616+0000</td></tr><tr><td>3</td><td>spring green yellow purple cornsilk</td><td>Manufacturer#4</td><td>Brand#42</td><td>STANDARD POLISHED BRASS</td><td>21</td><td>WRAP CASE</td><td>903.00</td><td>egular deposits hag</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:46:06.616+0000</td></tr><tr><td>4</td><td>cornflower chocolate smoke green pink</td><td>Manufacturer#3</td><td>Brand#34</td><td>SMALL PLATED BRASS</td><td>14</td><td>MED DRUM</td><td>904.00</td><td>p furiously r</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:46:06.616+0000</td></tr><tr><td>5</td><td>forest brown coral puff cream</td><td>Manufacturer#3</td><td>Brand#32</td><td>STANDARD POLISHED TIN</td><td>15</td><td>SM PKG</td><td>905.00</td><td> wake carefully </td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:46:06.616+0000</td></tr><tr><td>6</td><td>bisque cornflower lawn forest magenta</td><td>Manufacturer#2</td><td>Brand#24</td><td>PROMO PLATED STEEL</td><td>4</td><td>MED BAG</td><td>906.00</td><td>sual a</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:46:06.616+0000</td></tr><tr><td>7</td><td>moccasin green thistle khaki floral</td><td>Manufacturer#1</td><td>Brand#11</td><td>SMALL PLATED COPPER</td><td>45</td><td>SM BAG</td><td>907.00</td><td>lyly. ex</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:46:06.616+0000</td></tr><tr><td>8</td><td>misty lace thistle snow royal</td><td>Manufacturer#4</td><td>Brand#44</td><td>PROMO BURNISHED TIN</td><td>41</td><td>LG DRUM</td><td>908.00</td><td>eposi</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:46:06.616+0000</td></tr><tr><td>9</td><td>thistle dim navajo dark gainsboro</td><td>Manufacturer#4</td><td>Brand#43</td><td>SMALL BURNISHED STEEL</td><td>12</td><td>WRAP CASE</td><td>909.00</td><td>ironic foxe</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:46:06.616+0000</td></tr><tr><td>10</td><td>linen pink saddle puff powder</td><td>Manufacturer#5</td><td>Brand#54</td><td>LARGE BURNISHED STEEL</td><td>44</td><td>LG CAN</td><td>910.01</td><td>ithely final deposit</td><td>dbfs:/databricks-datasets/tpch/data-001/part/part.tbl</td><td>2022-10-05T00:46:06.616+0000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 5-2. 監査列の付与方法"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c92c2f92-f8bf-4dd1-839b-99ed2f4b83d9"}}},{"cell_type":"markdown","source":["付与する監査列を検討を行い、その監査列を付与する関数の利用がおすすめ。<br>\n本ノートブックでは次のカラム付与を行っている。\n\n| #    | カラム名          | 概要                                                 |\n| ---- | ----------------- | ---------------------------------------------------- |\n| 1    | _datasource       | 取り込みファイルの完全パス                           |\n| 2    | _ingest_timestamp | データ取り込みを実施した日付時刻（グリニッジ標準時） |"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a28918c-e1aa-4788-ad9c-ac5c5aa77055"}}},{"cell_type":"code","source":["def with_audit_cols(tgt_df):\n    return (tgt_df\n        .withColumn(\"_datasource\", input_file_name())\n        .withColumn(\"_ingest_timestamp\", current_timestamp())\n     )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"660cd666-05fe-4711-af43-dc5b811560ec"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### 5-3. UDF（User-defined scalar functions）の利用方針"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d231dc88-1796-408d-a80b-eb4bc951f265"}}},{"cell_type":"markdown","source":["性能のボトルネットとなるため、基本的には利用しないこと。利用する場合には、UDFの管理業務フローを含めて設計すること。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"110d0094-0b34-401f-8a0f-2179218d4222"}}},{"cell_type":"markdown","source":["## 6. データエンジニアリングにおける Spark 関連機能"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9653c882-b08b-4fce-8aa5-df3894b06cfd"}}},{"cell_type":"markdown","source":["### 6-1. Spark コア機能"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a966f750-a1f8-44d4-866a-fe81aff685ba"}}},{"cell_type":"markdown","source":["#### 1. データフレーム、および、データストリーム時におけるデータソースごとのパラメータ"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d621a53-688e-4b16-b721-c82c24eadcfb"}}},{"cell_type":"markdown","source":["CSV や JSON などのデータソースごとに設定できるパラメータが異なるため、Spark Dcos 、および、PySpark Docs にて確認する。\n\n-   Spark Docs\n    -   [CSV Files - Spark 3.3.0 Documentation (apache.org)](https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option)\n    -   [JSON Files - Spark 3.3.0 Documentation (apache.org)](https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option)\n    -   [Parquet Files - Spark 3.3.0 Documentation (apache.org)](https://spark.apache.org/docs/latest/sql-data-sources-parquet.html)\n    -   [Apache Avro Data Source Guide - Spark 3.3.0 Documentation](https://spark.apache.org/docs/latest/sql-data-sources-avro.html)\n    -   [JDBC To Other Databases - Spark 3.3.0 Documentation (apache.org)](https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html#data-source-option)\n-   PySpark Docs\n    -   CSV\n        -   [pyspark.sql.DataFrameReader.csv — PySpark 3.3.0 documentation (apache.org)](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html)\n        -   [pyspark.sql.DataFrameWriter.csv — PySpark 3.3.0 documentation (apache.org)](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.csv.html)\n        -   [pyspark.sql.streaming.DataStreamReader.csv — PySpark 3.3.0 documentation (apache.org)](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamReader.csv.html)\n    -   JSON\n        -   [pyspark.sql.DataFrameReader.json — PySpark 3.3.0 documentation (apache.org)](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.json.html)\n        -   [pyspark.sql.DataFrameWriter.json — PySpark 3.3.0 documentation (apache.org)](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.json.html)\n        -   [pyspark.sql.streaming.DataStreamReader.json — PySpark 3.3.0 documentation (apache.org)](https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/api/pyspark.sql.streaming.DataStreamReader.json.html)\n\n\nJDBC 、Spark コネクターについては、提供元のドキュメント等にて確認する。\n\n-   [JDBC ドライバーの接続パラメーターリファレンス — Snowflake Documentation](https://docs.snowflake.com/ja/user-guide/jdbc-parameters.html)\n-   [Sparkコネクタの使用 — Snowflake Documentation](https://docs.snowflake.com/ja/user-guide/spark-connector-use.html#setting-configuration-options-for-the-connector)\n-   [接続プロパティの設定 - JDBC Driver for SQL Server | Microsoft Docs](https://docs.microsoft.com/ja-jp/sql/connect/jdbc/setting-the-connection-properties?view=sql-server-ver16)\n-   [SQL Server 用の Apache Spark コネクタ - Spark connector for SQL Server | Microsoft Docs](https://docs.microsoft.com/ja-jp/sql/connect/spark/connector?view=sql-server-ver16#supported-options)\n-   [Connecting to the Database (postgresql.org)](https://jdbc.postgresql.org/documentation/head/connect.html)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16db76bd-3a93-4b83-a568-aefcd1864eba"}}},{"cell_type":"markdown","source":["#### 2. Spark テーブルにおける外部テーブルとマネージドテーブルの利用指針"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f7713ec-5daa-463e-af55-4eb79716e447"}}},{"cell_type":"markdown","source":["Spark テーブルには、外部テーブル（アンマネージドテーブル）とマネージドテーブルがあるが、基本的には外部テーブルをを利用すること。<br>\nテスト実行時や短期的な利用を目的とする場合には、マネージドテーブルを利用してもよい。マネージドテーブルのロケーションを参照した外部テーブルを作成することも可能。\n\nマネージドテーブルを削除する際には、データサイズによっては事前にデータを削除する必要あり。\n\n- [マネージド Delta Lake テーブルの削除のベスト プラクティス - Azure Databricks | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/databricks/kb/delta/drop-delta-table)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a9a90f3f-80e2-47db-a993-6fbd3f50ffe8"}}},{"cell_type":"code","source":["def get_table_location(\n    tgt_full_name: str,\n):\n    spark = SparkSession.getActiveSession()\n\n    location = (\n        spark.sql(f\"DESC TABLE EXTENDED {tgt_full_name}\")\n        .filter('col_name = \"Location\"')\n        .select(\"data_type\")\n        .collect()[0][0]\n    )\n    return location"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c54da9c2-2553-42b4-b3cd-102c6790a0df"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["db_name = 'sample_tpch'\ntbl_name = 'orders'\ntbl_full_name = f'{db_name}.{tbl_name}'\n\nspark.sql(f'''\nCREATE OR REPLACE TABLE {tbl_full_name}\n(\n    o_orderkey long,\n    o_custkey long,\n    o_orderstatus string,\n    o_totalprice decimal(12, 2),\n    o_orderdate date,\n    o_orderpriority string,\n    o_clerk string,\n    o_shippriority int,\n    o_comment string\n)\nUSING delta\n''')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2f57971-a6b2-4041-8f41-6b4b375204a1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[75]: DataFrame[]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[75]: DataFrame[]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# tbl_location を取得\ntbl_location = get_table_location(tbl_full_name)\nprint(tbl_location)\nspark.table(f\"delta.`{tbl_location}`\").printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f541906d-fbb0-4c33-90a4-aa27956210d6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">dbfs:/user/hive/warehouse/sample_tpch.db/orders\nroot\n |-- o_orderkey: long (nullable = true)\n |-- o_custkey: long (nullable = true)\n |-- o_orderstatus: string (nullable = true)\n |-- o_totalprice: decimal(12,2) (nullable = true)\n |-- o_orderdate: date (nullable = true)\n |-- o_orderpriority: string (nullable = true)\n |-- o_clerk: string (nullable = true)\n |-- o_shippriority: integer (nullable = true)\n |-- o_comment: string (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">dbfs:/user/hive/warehouse/sample_tpch.db/orders\nroot\n-- o_orderkey: long (nullable = true)\n-- o_custkey: long (nullable = true)\n-- o_orderstatus: string (nullable = true)\n-- o_totalprice: decimal(12,2) (nullable = true)\n-- o_orderdate: date (nullable = true)\n-- o_orderpriority: string (nullable = true)\n-- o_clerk: string (nullable = true)\n-- o_shippriority: integer (nullable = true)\n-- o_comment: string (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# マネージドテーブルの Location を参照した外部テーブルを作成\ndb_name = 'sample_tpch'\ntbl_name_2 = 'orders_2'\ntbl_full_name_2 = f'{db_name}.{tbl_name_2}'\n\nspark.sql(f'''\nDROP TABLE IF EXISTS {tbl_full_name_2}\n''')\nspark.sql(f'''\nCREATE TABLE {tbl_full_name_2}\nUSING delta\nLOCATION '{tbl_location}'\n''')\n\nprint(spark.sql(f'SHOW CREATE TABLE {tbl_full_name_2}').collect()[0][0])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"20ce70dd-0c7b-4f7b-8bd5-e9e9f56b4553"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">CREATE TABLE spark_catalog.sample_tpch.orders_2 (\n  o_orderkey BIGINT,\n  o_custkey BIGINT,\n  o_orderstatus STRING,\n  o_totalprice DECIMAL(12,2),\n  o_orderdate DATE,\n  o_orderpriority STRING,\n  o_clerk STRING,\n  o_shippriority INT,\n  o_comment STRING)\nUSING delta\nLOCATION &#39;dbfs:/user/hive/warehouse/sample_tpch.db/orders&#39;\nTBLPROPERTIES (\n  &#39;Type&#39; = &#39;EXTERNAL&#39;,\n  &#39;delta.minReaderVersion&#39; = &#39;1&#39;,\n  &#39;delta.minWriterVersion&#39; = &#39;2&#39;)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">CREATE TABLE spark_catalog.sample_tpch.orders_2 (\n  o_orderkey BIGINT,\n  o_custkey BIGINT,\n  o_orderstatus STRING,\n  o_totalprice DECIMAL(12,2),\n  o_orderdate DATE,\n  o_orderpriority STRING,\n  o_clerk STRING,\n  o_shippriority INT,\n  o_comment STRING)\nUSING delta\nLOCATION &#39;dbfs:/user/hive/warehouse/sample_tpch.db/orders&#39;\nTBLPROPERTIES (\n  &#39;Type&#39; = &#39;EXTERNAL&#39;,\n  &#39;delta.minReaderVersion&#39; = &#39;1&#39;,\n  &#39;delta.minWriterVersion&#39; = &#39;2&#39;)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 外部テーブルを削除しても、マネージドテーブルは残っている\nspark.sql(f'''\nDROP TABLE IF EXISTS {tbl_full_name_2}\n''')\n\nspark.table(f\"delta.`{tbl_location}`\").printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9aa7bb75-1b7c-4cdf-8c6d-6162a42c56c1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- o_orderkey: long (nullable = true)\n |-- o_custkey: long (nullable = true)\n |-- o_orderstatus: string (nullable = true)\n |-- o_totalprice: decimal(12,2) (nullable = true)\n |-- o_orderdate: date (nullable = true)\n |-- o_orderpriority: string (nullable = true)\n |-- o_clerk: string (nullable = true)\n |-- o_shippriority: integer (nullable = true)\n |-- o_comment: string (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- o_orderkey: long (nullable = true)\n-- o_custkey: long (nullable = true)\n-- o_orderstatus: string (nullable = true)\n-- o_totalprice: decimal(12,2) (nullable = true)\n-- o_orderdate: date (nullable = true)\n-- o_orderpriority: string (nullable = true)\n-- o_clerk: string (nullable = true)\n-- o_shippriority: integer (nullable = true)\n-- o_comment: string (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### 3. シークレットの利用"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"87c079c8-57ad-4de2-a40f-8ac4df51b0fe"}}},{"cell_type":"markdown","source":["Spark を利用する際には、パスワード等のシークレットが平文で表示されないように、Spark プロバイダーが提供するシークレット管理機能を用いる必要がある。\n\n-   [シークレットの管理 - Azure Databricks | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/databricks/security/secrets/)\n-   [資格情報ユーティリティ | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-python#credentials-utilities)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a7e943b-73c2-4158-b3e4-a9a24430ce4e"}}},{"cell_type":"markdown","source":["#### 4. サロゲートキー列の付与"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f8dd1f5b-84ed-4ec6-8918-b4ff3b946d46"}}},{"cell_type":"markdown","source":["サロゲートキー列を付与する場合には、プログラムの複雑度と要求性能に応じて、次のいずれの方法での実装を検討。<br>\n自テーブルのサロゲートキー列の値を他テーブルに保持させる場合には、プログラムが複雑になる可能性がある。\n\n| #    | 実装方法                                   | サロゲートキーカラムのデータ型 | 再現性 |\n| ---- | ------------------------------------------ | ------------------------------ | ------ |\n| 1    | 主キー項目によるハッシュ値を利用する方法   | 文字列                         | 有     |\n| 2    | Delta Lake の Identity Column 機能を利用する方法     | 数値型                         | 無   |\n| 3    | テーブルへの書き込み時に連番を付与する方法 | 数値型                         | 無   |\n\n<br>\n\n- 関連リンク\n  - 主キー項目によるハッシュ値を利用する方法\n    - [pyspark.sql.functions.sha2 — PySpark 3.1.3 documentation (apache.org)](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.sha2.html)\n    - [sha2 関数 (Databricks SQL)-Azure Databricks - Databricks SQL | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/databricks/sql/language-manual/functions/sha2)\n  - Delta Lake の Identity Column 機能を利用する方法\n    - [CREATE TABLE [USING] (Databricks SQL) - Azure Databricks - Databricks SQL | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/databricks/sql/language-manual/sql-ref-syntax-ddl-create-table-using#parameters)\n  - テーブルへの書き込み時に連番を付与する方法\n    - [一意の増加する数値を生成する - Azure Databricks | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/databricks/kb/sql/gen-unique-increasing-values)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cefa0f6f-89ad-4a68-80e5-7485800bf464"}}},{"cell_type":"markdown","source":["#### 5. Spark におけるタイムスタンプ型カラムの仕様"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"018a5e93-3233-4bf4-a787-60af53815f34"}}},{"cell_type":"markdown","source":["Spark におけるタイムスタンプ型には、次の注意事項がある。\n\n- タイムゾーンのオフセットは、Spark で処理する際に損失し、Spark セッションのタイムゾーンに統一される。\n- データ取得元のタイムゾーンを保持する場合には、別カラムにタイムゾーンの値を保持する方法、あるいは、タイムゾーンのオフセットを加減した値を保持する方法のいずれかを実施する必要がある。\n- Spark では`TIMESTAMP WITH SESSION TIME ZONE`の timestamp 型のみがサポートさていることから、後者の方法を採用する場合には`spark.sql.session.timeZone`の固定化が必要となる。\n\n参考リンク\n- [日付とタイムスタンプ - Azure Databricks | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/databricks/spark/latest/dataframes-datasets/dates-timestamps)\n- [pyspark.sql.functions.from_utc_timestamp — PySpark 3.1.3 documentation (apache.org)](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.from_utc_timestamp.html)\n\n\n\nOracle database などのデータベースでは TIMESTAMP WITH TIME ZONE がサポートされており、Spark からデータを取得する場合には注意が必要となる。\n\n- [日時データ型とタイム・ゾーン・サポート (oracle.com)](https://docs.oracle.com/cd/E16338_01/server.112/b56307/ch4datetime.htm#i1006081)\n- [Getting py4j.protocol.Py4JJavaError: An error occurred while calling o65.jdbc. : java.sql.SQLException: Unsupported type TIMESTAMP_WITH_TIMEZONE (denodo.com)](https://community.denodo.com/answers/question/details?questionId=9064u000000HAePAAW)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c3facde-af08-4522-9212-8e86b661e538"}}},{"cell_type":"code","source":["# 事前準備\nfrom pyspark.sql.functions import from_utc_timestamp,col\ndf = spark.createDataFrame(\n        [\n            (2020, 6, 28, 10, 31, 30, 'UTC'),\n            (2019, 3, 1, 0, 1, 2, 'America/Los_Angeles'), \n            (2019, 11, 3, 1, 30, 2, 'America/Los_Angeles'), \n            (2019, 2, 28, 9, 29, 1, 'Asia/Tokyo'),\n        ],\n        ['YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTE', 'SECOND', 'TZ']\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e69e507-1d4c-4f68-ad29-c9a89eca1b92"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# デフォルトのタイムゾーンを確認\nspark.conf.unset('spark.sql.session.timeZone')\nprint(spark.conf.get('spark.sql.session.timeZone'))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02d94edc-8917-469c-aa10-365a0a9944d8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Etc/UTC\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Etc/UTC\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# デフォルトのタイムゾーンに統一される\ndf.selectExpr(\n    'make_timestamp(YEAR, MONTH, DAY, HOUR, MINUTE, SECOND, TZ) as timestamp_with_session_time_zone',\n).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfa91d29-61e3-4545-8314-4642e718d945"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["2020-06-28T10:31:30.000+0000"],["2019-03-01T08:01:02.000+0000"],["2019-11-03T08:30:02.000+0000"],["2019-02-28T00:29:01.000+0000"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"timestamp_with_session_time_zone","type":"\"timestamp\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>timestamp_with_session_time_zone</th></tr></thead><tbody><tr><td>2020-06-28T10:31:30.000+0000</td></tr><tr><td>2019-03-01T08:01:02.000+0000</td></tr><tr><td>2019-11-03T08:30:02.000+0000</td></tr><tr><td>2019-02-28T00:29:01.000+0000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# タイムゾーンを指定すると、それに統一される\nspark.conf.set('spark.sql.session.timeZone', 'Asia/Tokyo')\n\ndf.selectExpr(\n    'make_timestamp(YEAR, MONTH, DAY, HOUR, MINUTE, SECOND, TZ) as timestamp_with_session_time_zone',\n).display()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"36ad3b16-8af9-4749-baf7-bda7468fcc43"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["2020-06-28T19:31:30.000+0900"],["2019-03-01T17:01:02.000+0900"],["2019-11-03T17:30:02.000+0900"],["2019-02-28T09:29:01.000+0900"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"timestamp_with_session_time_zone","type":"\"timestamp\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>timestamp_with_session_time_zone</th></tr></thead><tbody><tr><td>2020-06-28T19:31:30.000+0900</td></tr><tr><td>2019-03-01T17:01:02.000+0900</td></tr><tr><td>2019-11-03T17:30:02.000+0900</td></tr><tr><td>2019-02-28T09:29:01.000+0900</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["別カラムにタイムゾーンの値を保持する方法を実施する方法の例"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c383772a-e2cd-47b0-961f-ade9e9a155c6"}}},{"cell_type":"code","source":["spark.conf.unset('spark.sql.session.timeZone')\n(\n    df.selectExpr(\n        'make_timestamp(YEAR, MONTH, DAY, HOUR, MINUTE, SECOND, TZ) as timestamp_with_session_time_zone',\n        'TZ AS time_zone_id',\n    )\n    .display()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a8fb81e0-6b4e-4228-b2f0-f2c6287848c4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["2020-06-28T10:31:30.000+0000","UTC"],["2019-03-01T08:01:02.000+0000","America/Los_Angeles"],["2019-11-03T08:30:02.000+0000","America/Los_Angeles"],["2019-02-28T00:29:01.000+0000","Asia/Tokyo"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"timestamp_with_session_time_zone","type":"\"timestamp\"","metadata":"{}"},{"name":"time_zone_id","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>timestamp_with_session_time_zone</th><th>time_zone_id</th></tr></thead><tbody><tr><td>2020-06-28T10:31:30.000+0000</td><td>UTC</td></tr><tr><td>2019-03-01T08:01:02.000+0000</td><td>America/Los_Angeles</td></tr><tr><td>2019-11-03T08:30:02.000+0000</td><td>America/Los_Angeles</td></tr><tr><td>2019-02-28T00:29:01.000+0000</td><td>Asia/Tokyo</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["タイムゾーンのオフセットを加減した値を保持する方法の例"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d44163d9-b036-4796-a379-7a5d8f9eac7d"}}},{"cell_type":"code","source":["spark.conf.unset('spark.sql.session.timeZone')\n(\n    df\n    .selectExpr(\n        'make_timestamp(YEAR, MONTH, DAY, HOUR, MINUTE, SECOND, TZ) as timestamp_with_session_time_zone',\n        'TZ AS time_zone_id',\n    )\n    .withColumn(\n        'timestamp_without_time_zone',\n        from_utc_timestamp(col('timestamp_with_session_time_zone'),col('time_zone_id'))\n    )\n    .drop('time_zone_id')\n    .display()\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e72f1f72-3cf1-4bc0-a2bf-c436b115c89d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["2020-06-28T10:31:30.000+0000","2020-06-28T10:31:30.000+0000"],["2019-03-01T08:01:02.000+0000","2019-03-01T00:01:02.000+0000"],["2019-11-03T08:30:02.000+0000","2019-11-03T01:30:02.000+0000"],["2019-02-28T00:29:01.000+0000","2019-02-28T09:29:01.000+0000"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"timestamp_with_session_time_zone","type":"\"timestamp\"","metadata":"{}"},{"name":"timestamp_without_time_zone","type":"\"timestamp\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>timestamp_with_session_time_zone</th><th>timestamp_without_time_zone</th></tr></thead><tbody><tr><td>2020-06-28T10:31:30.000+0000</td><td>2020-06-28T10:31:30.000+0000</td></tr><tr><td>2019-03-01T08:01:02.000+0000</td><td>2019-03-01T00:01:02.000+0000</td></tr><tr><td>2019-11-03T08:30:02.000+0000</td><td>2019-11-03T01:30:02.000+0000</td></tr><tr><td>2019-02-28T00:29:01.000+0000</td><td>2019-02-28T09:29:01.000+0000</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### 6. 破損したファイル（corrupt files）を参照する場合の対応方法"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"748821c9-3188-4ffd-b32e-1a2924c0ff2b"}}},{"cell_type":"markdown","source":["CSV や JSON などのファイルを適切に読めこめないことをファイル破損（corrupt files）と言われており、PySpark では次の方法により破損理由を探すことができる。\n\n- ファイル内容を確認する方法\n  - [Text Files - Spark 3.3.0 Documentation (apache.org)](https://spark.apache.org/docs/latest/sql-data-sources-text.html)\n  - [dbutils.fs.head](https://docs.microsoft.com/ja-jp/azure/databricks/dev-tools/databricks-utils#dbutils-fs)\n- データ型が適切であるかを確認する方法\n  - [Databricks（Spark）にてDataFrameReaderのmode機能を用いたエラーレコードの抽出方法 - Qiita](https://qiita.com/manabian/items/827601a421b4bde297c8)\n\n\n区切りテキストファイル（CSV、TSV） のファイル破損（corrupt CSV files）の原因としては次のようなものがある。\n\n- 全体の設定値が適切でないこと\n  - ファイルの文字エンコーディング（encoding）（例： UTF-8 、 Shift_JIS ）が適切でない\n  - 区切り文字（separator）が適切でない\n  - 改行コード（delimiter）が適切でない\n  - テキスト修飾子（quote）が適切でない\n  - テキスト修飾子のエスケープ（escape）が適切でない\n  - 改行コード（line separator）適切でない\n  - ヘッダーの有無（header）が適切でない\n  - ファイル圧縮の形式（compression）が適切でない\n- 個別の値が適切でないこと\n  - テキスト修飾子が指定されてない場合に、改行コードが値に含まれている\n  - テキスト修飾子のエスケープが追記されていないテキスト修飾子が値に含まれている\n  - 想定のデータ型と一致していない\n    - 桁数が想定値を上回っている\n    - 数値型である値に、`,` や `$`などの記号が記載されている\n    - 日付型のフォーマット（dateFormat）、あるいは、タイムスタンプ型のフォーマット（timestampFormat）が異なる\n    - ヘッダーが有無の設定が適切でなく、ヘッダーのカラム名がレコードとして認識されている\n    \n区切りテキストファイルにおける設定値は、次のリンクが参考となる。\n\n-  [Data Source Option](https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dcbe29eb-2149-42a1-a343-69a5d0f1cc22"}}},{"cell_type":"markdown","source":["### 6-2. Delta Lake 機能"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e62d3ee-eef1-4054-8646-0be708bff451"}}},{"cell_type":"markdown","source":["#### 1. Delta Lake におけるタイムトラベル機能"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"594c748e-397c-4871-a41a-38bfb1ebe3b2"}}},{"cell_type":"markdown","source":["Delta Lake形式のテーブルでは、parquet ファイルが追記される仕様であることから、過去の時点のバージョンへのクエリ（タイムトラベル）が可能。\n\n保存期間は次のパラメータと VACUUM 操作に依存。\n\n- delta.logRetentionDuration\n- delta.deletedFileRetentionDuration\n\n参考リンク\n\n- [データの保持 - Azure Databricks | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/databricks/delta/delta-batch#data-retention)\n\n誤って書き込みを実施してしまったデータを削除する場合には、追加のオペレーションが必要となる場合がある。\n\n- [ベスト プラクティス:Delta Lake を使用した GDPR と CCPA のコンプライアンス - Azure Databricks | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/databricks/security/privacy/gdpr-delta)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"983997c5-6607-48ee-ae9b-c52f55f87604"}}},{"cell_type":"markdown","source":["#### 2. Delta Lake における変更データフィード機能"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2301e4b7-9d16-4b1f-b9ba-2da95028379b"}}},{"cell_type":"markdown","source":["変更データフィードを有効にすることで行レベルでの追跡が有効となるため、基本的には有効とすること。\n\n設定による性能への大きな影響はないとの記載あり。\n\n> 変更データ フィードを有効にするオーバーヘッドはありますか。\n> 大きな影響はありません。\n\n引用元：[データ フィードの変更 - Azure Databricks | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/databricks/delta/delta-change-data-feed#what-is-the-overhead-of-enabling-the-change-data-feed)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"004ee0f5-25a4-4698-b938-b31c34a05649"}}},{"cell_type":"markdown","source":["#### 3. Delta Lake における CLONE 機能"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8f7a7b5d-b6d6-4586-afae-959045c817a5"}}},{"cell_type":"markdown","source":["マルチクラウド・マルチリージョンでの展開を行う際に、増分更新が実際されることもあり、データ書き込み後のDEEP CLONE が有効。\n\n2022年7月25日時点で、CLONE は Databricks でのみ利用でき、OSS の Delta Lake に実装予定。"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"31ad5ad5-52ce-4929-bed5-dec23fffcdcd"}}},{"cell_type":"markdown","source":["#### 4. Delta Lake における Delta Sharing 機能"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e9393ab-b05f-4341-81fd-b432ee953e33"}}},{"cell_type":"code","source":["# Delta Sharing 機能が GA 後に検証予定"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7cb1f355-76d4-4725-9d5e-6afc73c2b03f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 7. パフォーマン最適化"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8eede1e-962c-4e80-ab82-897c01d98d9b"}}},{"cell_type":"markdown","source":["### 7-1. 他データストアと連携する場合にSpark コネクターを優先して利用"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4cb16f1-c52d-4a61-a7b3-b8fd626a8de4"}}},{"cell_type":"markdown","source":["Spark にてデータベースからデータ連携を行う場合には、jdbc や Python のライブラリーを利用せずに、Spark コネクターを利用すること。\n\n-   [Spark用Snowflakeコネクター — Snowflake Documentation](https://docs.snowflake.com/ja/user-guide/spark-connector.html)\n-   [SQL Server 用の Apache Spark コネクタ - Spark connector for SQL Server | Microsoft Docs](https://docs.microsoft.com/ja-jp/sql/connect/spark/connector?view=sql-server-ver16)\n-   [TD Python Spark Driver with Databricks - Product Documentation - Treasure Data Product Documentation](https://docs.treasuredata.com/display/public/PD/TD+Python+Spark+Driver+with+Databricks)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06bbfbdb-ecac-4e0e-a8cc-2875b1b95735"}}},{"cell_type":"markdown","source":["### 7-2. Delta Lakeにおけるパフォーマンス最適化"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2134a567-9228-44e3-90ca-faf4a6c443a2"}}},{"cell_type":"markdown","source":["次のようなドキュメントを参考に、テーブルプロパティやデータ連携前後の処理を検討。\n\n- 参考リンク\n  - [ファイル管理を使用してパフォーマンスを最適化する - Azure Databricks | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/databricks/delta/optimizations/file-mgmt)\n  - [自動最適化 - Azure Databricks | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/databricks/delta/optimizations/auto-optimize)\n  - [ANALYZE TABLE - Azure Databricks | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/databricks/spark/latest/spark-sql/language-manual/sql-ref-syntax-aux-analyze-table)\n  - [VACUUM - Azure Databricks | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/databricks/spark/latest/spark-sql/language-manual/delta-vacuum)\n  - [What's the best practice on running ANALYZE on Delta Tables for query performance optimization? (databricks.com)](https://community.databricks.com/s/question/0D53f00001GHVicCAH/whats-the-best-practice-on-running-analyze-on-delta-tables-for-query-performance-optimization)\n\n\n- 設定例\n  - テーブルプロパティ\n    - delta.autoOptimize.optimizeWrite\n    - delta.autoOptimize.autoCompact\n    - delta.dataSkippingNumIndexedCols\n  - データ連携後に行うべき処理\n    - Optimnize\n    - Z-order\n    - Vacuum\n    - Analyze table"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c1e054b3-94f6-453f-895b-54c884116f46"}}},{"cell_type":"markdown","source":["### 7-3. データエンジニアリング時におけるクラスターの利用指針"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a4af9e79-cdd9-4ba6-9271-37e1e9a19a84"}}},{"cell_type":"markdown","source":["分散処理でデータエンジニアリングを行う場合には、複数台の汎用的なサーバーにて処理を行われることがあるが、次のような記載がある通り、小数のサーバーの方が性能が高くなることがある。\n\n> 複数のテーブルにまたがる和集合や結合を必要とする処理など、より複雑な ETL ジョブは、シャッフルされるデータ量を最小限に抑えることができれば最適に実行されます。 クラスター内のワーカー数を減らすことでシャッフルを最小限に抑えることができるため、クラスター D のような大規模なクラスターよりも、次の図のクラスター A のような小さなクラスターを検討することをお勧めします。\n\n引用元：[複雑なバッチ ETL | Microsoft Docs](https://docs.microsoft.com/ja-jp/azure/databricks/clusters/cluster-config-best-practices#--complex-batch-etl)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc7383b6-e780-45e6-ab55-e3303bca0421"}}},{"cell_type":"markdown","source":["## リソースのクリーンアップ"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"03305ce7-e2a5-467e-a931-2367ee4bf971"}}},{"cell_type":"code","source":["db_name = 'sample_tpch'\n\nspark.sql(f'''\nDROP DATABASE {db_name} CASCADE\n''')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f9cdcf0-9f67-4ce4-b078-e5e638009225"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[86]: DataFrame[]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[86]: DataFrame[]</div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"T20__L100__010","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":1640450761247422,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":1640450761247283}},"nbformat":4,"nbformat_minor":0}
